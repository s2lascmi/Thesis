Meine Preprocessing-Schritte:

- Einlesen der Texte im PDF und XML Format
- Bereinigen der Texte von Abkürzungen, https-links, XML-Auszeichnungen, weiterer Zeichensetzung
- Entfernen von Stopwörtern sowie sehr langen und sehr kurzen Wörtern, basierend auf der Textsprache (DE oder EN)
- Lemmatisierung der Texte: Nur Nomen, Verben, Adjektive und Adverben bleiben erhalten, Lemmatisierung je nach festgestellter Sprache
- Kleinschreibung der Wörter
- Bigramme und Trigramme erstellen
- Bag of Words und TF-IDF weighting


Weitere Schritte:
- Keywords und Autorenteams aus den XML-Dateien extrahieren für FF 4, 5, 6

Topic Modeling:
- zunächst optimale Anzahl k der Topics feststellen
- danach andere Parameter optimieren: Alpha, Eta, Update_every
--> daraus das beste TM auswählen und für den Rest der MA verwenden


FF1:
- eigentliches Topic Modeling und Analyse der entstandenen Topics
FF2:
- Hierarchiches Clustering der Texte auf Basis der Hellinger Distanz
FF3:
- Mann-Kendall-Test, um Trends in der Entwicklung der Topics festzustellen (werden sie häufiger oder weniger häufig im Verlauf?)
FF4:
- Forschungsmethoden werden analysiert auf Basis der Auszählungen der hinterlegten XML-Keywords
- ersten fünf aus jedem DHd-Jahr werden berücksichtigt?
FF5:
- Netzwerke/Autorenteams analysieren pro Jahr
FF6:
- bisher noch unbearbeitet



==================


