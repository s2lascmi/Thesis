{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code for Master's Thesis: Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. Welche Themen können mithilfe von Topic Modeling aus den DHd-Abstracts\n",
    "der Tagungen zwischen 2014 und 2023 gefunden werden?\n",
    "\n",
    "*Which topics can be found in the abstracts from DHd-conferences between 2014 and 2023 with Topic Modeling?*\n",
    "\n",
    "2. Welche Themen kommen häufig gemeinsam in einem Dokument vor und weisen\n",
    "daher eine hohe Themenähnlichkeit (topic similarity) auf?\n",
    "\n",
    "*Which topics appear frequently in one abstract and therefore have a high topic similarity?* **Hierarchical Clustering**\n",
    "\n",
    "3. Wie haben sich die Themenschwerpunkte im Verlauf der Jahre verändert -\n",
    "welche Trends sind zu erkennen?\n",
    "\n",
    "*How have the topics been changing throughout the years - which trends are perceptible?* **Mann-Kendall-Test**\n",
    "\n",
    "4. Welche Entwicklungen sind in Bezug auf die Verwendung verschiedener Forschungsmethoden festzustellen?\n",
    "\n",
    "*With regard to the use of different scientific methods, which developments are perceptible?*\n",
    "\n",
    "5. Welche Personen sind besonders häufig mit Abstracts vertreten, in welchen\n",
    "Autor:innenteams treten sie auf und wie verändern sich diese im Zeitverlauf?\n",
    "\n",
    "*Which researchers contribute to the conference particularly frequently with abstracts, in which teams do they contribute and how have the teams been changing?*\n",
    "\n",
    "6. Welche Personencluster sind in Bezug auf die Themenschwerpunkte zu erkennen und wie verändern sich diese?\n",
    "\n",
    "*Which clusters of researchers can be found with regard to topics and how have the clusters been changing?* **Network Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in necessary pdf- and xml-files\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "'''Vermerken: PyPDF2 hat die Zeichen nicht gut erkannt und daher sind einige Wörter herausgefallen'''\n",
    "import PyPDF2\n",
    "import fitz\n",
    "from io import BytesIO\n",
    "\n",
    "#(pre)processing the files\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from gensim.models import TfidfModel\n",
    "import pickle\n",
    "''' In-Script Funktion hat nicht funktioniert '''\n",
    "from ocrfixr import spellcheck\n",
    "\n",
    "#LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions: opening lists, saving and reopening objects, function to get conference names from file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list(doc_name):\n",
    "    file = open(doc_name, \"r\", encoding='utf-8')\n",
    "    data = file.read()\n",
    "\n",
    "    data = data.split(\", \")\n",
    "    return data\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(dirname, filename, varname):\n",
    "    filename = dirname + filename\n",
    "    g = open(filename, 'wb')\n",
    "    pickle.dump(varname, g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_variable(dirname, filename):\n",
    "    path = str(dirname) + str(filename)\n",
    "    f = open(path, 'rb')\n",
    "    filename = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conference_names(list):\n",
    "    files = []\n",
    "    for element in list:\n",
    "        new_name = element.split('.zip')[0]\n",
    "        new_name = new_name.split('Corpus/')[1]\n",
    "        new_name = re.sub('_', ' ', new_name)\n",
    "        files.append(new_name)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: determining text language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \n",
    "    #gets text as input\n",
    "    lang = detect(text)\n",
    "\n",
    "    #returns the language tag of detected language\n",
    "    return lang        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \n",
    "    # filtering paragraphs from text\n",
    "    clean = re.sub(r'\\n', \"\", str(text))\n",
    "\n",
    "    \n",
    "    # filtering weblinks, digits and markup from XML\n",
    "    abbreviations = [r'http(.*?) ', r'\\d', r'<(.*?)>', r'https(.*?) ']\n",
    "    for word in abbreviations:\n",
    "        clean = re.sub(word, '', clean)\n",
    "    \n",
    "    # filtering punctuation\n",
    "    punctuation = '''!“()´`¨[]{}\\\\;:”\",<>/.?@#$%^&*_~''' \n",
    "    for word in clean:\n",
    "        if word in punctuation:\n",
    "            clean = clean.replace(word, \"\")\n",
    "    \n",
    "    # convert a document into a list of lowercase tokens, ignoring tokens that are too short (min_len=2) or too long (max_len=15), no deaccentation (by default)\n",
    "    clean = gensim.utils.simple_preprocess(clean, min_len=3, max_len=25)\n",
    "\n",
    "    # returns cleaned-up texts\n",
    "    return clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: removing stopwords and very short/long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, language, additional_stops):\n",
    "    \n",
    "    # import German stopword list \n",
    "    stops_de = set(stopwords.words('german'))\n",
    "    stops_de.update(additional_stops)\n",
    "    \n",
    "    stops_en = set(stopwords.words('english'))\n",
    "    stops_en.update(additional_stops)\n",
    "    \n",
    "    \n",
    "    # filter stopwords\n",
    "    words_filtered = []\n",
    "    for w in text:\n",
    "        if language == 'de':\n",
    "            if w not in stops_de:\n",
    "                words_filtered.append(w)\n",
    "\n",
    "        elif language == 'en':\n",
    "            if w not in stops_en:\n",
    "                words_filtered.append(w)\n",
    "    \n",
    "    # return list of words that are NOT stopwords\n",
    "    return words_filtered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: (morpho-syntactic) lemmatization\n",
    "- Lemmatizing the words in the texts to their dictionary form according to the detected language\n",
    "- Hint: 'de_core_news_md' and 'en_core_web_sm' models have to be downloaded via pip beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, language):\n",
    "    \n",
    "    # only words tagged as nouns, verbs, adjectives and adverbs should be considered\n",
    "    allowed_tags = ['NOUN', 'VERB', 'ADJ']\n",
    "\n",
    "    # disabling parser and ner-tool to accelerate computing \n",
    "    nlp_de = spacy.load('de_core_news_md', disable=['parser', 'ner'])\n",
    "    nlp_en = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        if language == 'de':\n",
    "            doc = nlp_de(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_tags:\n",
    "                    new_text.append(token.lemma_.lower())\n",
    "        elif language == 'en':\n",
    "            doc = nlp_en(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_tags:\n",
    "                    new_text.append(token.lemma_.lower())\n",
    "            \n",
    "        # delete all empty sets where the pos-tag was not in allowed list\n",
    "        if new_text != []:        \n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "    \n",
    "    # return list of lemmatized words\n",
    "    return (texts_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Extracting Keywords from XML-File\n",
    "- extracts tags \\<keywords n=\"topics\" scheme=\"ConfTool\"> and \\<keywords n=\"keywords\" scheme=\"ConfTool\"> to get keywords of the texts\n",
    "- checks validity of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    keywords = re.sub(\"<(.*?)>\", \"\", keywords)\n",
    "    keywords = keywords.split(\"\\n\")\n",
    "    for item in keywords:\n",
    "        if len(item) <= 2:\n",
    "            keywords.remove(item)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(xmldata, conf_tool_methods):\n",
    "    \n",
    "    # finds all tags <keywords n=\"keywords\"> and <keywords n=\"topics\">, removes all tags within\n",
    "    keywords_free= str(xmldata.find_all('keywords', n='keywords'))\n",
    "    keywords_conf = str(xmldata.find_all('keywords', n='topics'))\n",
    "    \n",
    "    keywords_free = clean_keywords(keywords_free)\n",
    "    keywords_conf = clean_keywords(keywords_conf)\n",
    "    for item in keywords_conf:\n",
    "        if item not in conf_tool_methods:\n",
    "            keywords_conf.remove(item)\n",
    "            \n",
    "    # returns list\n",
    "    return keywords_free, keywords_conf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Counting number of extracted keywords/authors/etc.\n",
    "- function creates dictionary from the input list\n",
    "- counts how often each method/author is used or appears\n",
    "- returns the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_appearances(input_list):\n",
    "    \n",
    "    methods_dict = {}\n",
    "    # for each item in keyword list, check if it is alredy in dictionary\n",
    "    # if not, add and set count to 1, if yes add +1 to count\n",
    "    for item in input_list:\n",
    "        if item not in methods_dict.keys():\n",
    "            methods_dict[item] = 1\n",
    "        else:\n",
    "            methods_dict[item] += 1\n",
    "    # sort dictionary according to highest count in the values\n",
    "    sorted_dict = sorted(methods_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # return the sorted dictionary (becomes list through sorting though)\n",
    "    return sorted_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Extracting the author names\n",
    "Extracts the names of the authors and returns a list of lists containing the names of the single texts' authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(title_stmt):\n",
    "    # returns a list of authors for each of the texts\n",
    "    \n",
    "    # navigating to the title statement and finding all tags <author>\n",
    "    authors = title_stmt.find_all(\"author\")\n",
    "    fore_and_surnames = []\n",
    "    \n",
    "    # extracting the <surname> and <forename> tags and cleaning the outcome from the tags and the brackets\n",
    "    for element in authors:\n",
    "        names = element.find_all(['surname', 'forename'])\n",
    "        names =  re.sub(\"<(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(\"</(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(r'\\]', \"\", names)\n",
    "        names = re.sub(r'\\[', \"\", names)\n",
    "        fore_and_surnames.append(names)\n",
    "    \n",
    "    return fore_and_surnames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Extracting text from XML-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_text(soup):\n",
    "    \n",
    "    # extract <p> tags from body of xml-document to find the actual text \n",
    "    document_body = soup.body\n",
    "    p_tags = document_body.find_all(\"p\")\n",
    "    \n",
    "    # return the text from p-tags\n",
    "    return p_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Extract List Items from Textfile After Manual OCR-Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_ocr(textfile):\n",
    "\n",
    "    pprocessed_pdf = []\n",
    "    pdf_ocr_corr = textfile.split(r']')\n",
    "    for item in pdf_ocr_corr:\n",
    "        item = re.sub(r', \\[', '', item)\n",
    "        item = re.sub(r'\\ufeff', '', item)\n",
    "        item = re.sub(r'\\[', '', item)\n",
    "        item = re.sub(r'\\'', '', item)\n",
    "        item = re.sub(r'\\’', '', item)\n",
    "        item = re.sub(r'\\‘', '', item)\n",
    "        clean = item.split(', ')\n",
    "        pprocessed_pdf.append(clean)\n",
    "    # cutting off the last item as it is not a text item\n",
    "    pprocessed_pdf = pprocessed_pdf[:-1]\n",
    "    \n",
    "    return pprocessed_pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Making bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts, bigram):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts, trigram,bigram):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams_trigrams(texts):\n",
    "   \n",
    "    bigram_phrases = gensim.models.Phrases(texts, min_count=8, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[texts], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    data_bigrams = make_bigrams(texts, bigram)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams, trigram, bigram)\n",
    "\n",
    "    return data_bigrams_trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Creating bag of words\n",
    "delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_bow(data_words): \n",
    "    \n",
    "#     # mapping the documents' words to a dictionary   \n",
    "#     id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "#     # creating a bag of words by using index of dictionary\n",
    "#     bag_of_words_corpus = []\n",
    "#     for text in data_words:\n",
    "#         new = id2word.doc2bow(text)\n",
    "#         bag_of_words_corpus.append(new)\n",
    "\n",
    "#     # returning id2word-reference as well as bag of word itself, both needed for LDA    \n",
    "#     return id2word, bag_of_words_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: TF-IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(id2word, texts):\n",
    "    # simple bag of words for each document, containing tuples with (index, number of appearances of the word in the document)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # calculates term frequency (TF) weighted by the inverse document frequency (IDF) for every word/index in the bag of words\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    # low_value as threshold\n",
    "    low_value = 0.03\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "\n",
    "    # for every single bag of words\n",
    "    for i in range(0, len(corpus)):\n",
    "        # consider each bow for each document\n",
    "        bow = corpus[i]\n",
    "        \n",
    "        # for each tuple (index, tfidf-value) in the tf-idf-weighted bag of words, extract index (tfidf_ids)\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        \n",
    "        # for each tuple (index, bow-value without tfidf), extract index\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        \n",
    "        # if the value in the (index, tfidf-value) tuple is lower than 0.03, put id into list low_value_words\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        \n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        \n",
    "        # which words will be deleted from the bow?\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "    \n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf score 0 will be missing\n",
    "        \n",
    "        # add words which indexes are not in low_value_words and not in words_missing_in_tfidf to the new bag of words \n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        \n",
    "        # new bow is missing certain indexes\n",
    "        corpus[i] = new_bow\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating repositories in which variables, models and figures can be saved later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Variables/\"):\n",
    "    os.mkdir('Variables/')\n",
    "    print('Created new directory: Variables')\n",
    "\n",
    "rqs = ['RQ1', 'RQ2', 'RQ3', 'RQ4', 'RQ5', 'RQ6', ]    \n",
    "for question in rqs:\n",
    "    if not os.path.isdir('Figures/'+question):\n",
    "        os.mkdir('Figures/'+question)\n",
    "        print('Created new directory: Figures/', question )\n",
    "    \n",
    "if not os.path.isdir('Models/'):\n",
    "    os.mkdir('Models/')\n",
    "    print('Created new directory: Models')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in zip-files of DHd-conferences where only PDF-files are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_pdf = ['Corpus/DHd_2014.zip', 'Corpus/DHd_2015.zip']\n",
    "doc_statistics = []\n",
    "\n",
    "# extracting text from pdf-files\n",
    "all_pdf_texts = []\n",
    "doc_names_pdf = []\n",
    "en_count = [0] * 9\n",
    "k = 0\n",
    "for conference_file in filenames_pdf:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    doc_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.pdf':\n",
    "            doc_names_year.append(name)\n",
    "            pdf_data = BytesIO(archive.read(name))\n",
    "            # reading each pdf-file in the zip-archive\n",
    "            with fitz.open(stream=pdf_data, filetype='pdf') as doc:\n",
    "                text = ''\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "\n",
    "                all_pdf_texts.append(text)\n",
    "                lang=detect_language(text)\n",
    "                if lang == 'en':\n",
    "                    en_count[k] += 1\n",
    "\n",
    "\n",
    "    doc_names_pdf.append(doc_names_year)\n",
    "    k += 1\n",
    "    \n",
    "\n",
    "filenames_pdf = get_conference_names(filenames_pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the zip-files of the DHd-Conferences where XML-files were published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_xml = ['Corpus/DHd_2016.zip', 'Corpus/DHd_2017.zip', 'Corpus/DHd_2018.zip', 'Corpus/DHd_2019.zip', 'Corpus/DHd_2020.zip',\n",
    "             'Corpus/DHd_2022.zip', 'Corpus/DHd_2023.zip']\n",
    "\n",
    "\n",
    "\n",
    "all_xml_files = []\n",
    "doc_names_xml = []\n",
    "# read in all zip-folders\n",
    "for conference_file in filenames_xml:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    xml_per_year = []\n",
    "    # read in all files in the zip-file and check that they are xml-files\n",
    "    doc_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.xml' and not name[-9:] == 'final.xml':\n",
    "            xml_per_year.append(archive.read(name))\n",
    "            doc_names_year.append(name)\n",
    "    all_xml_files.append(xml_per_year)\n",
    "    # creating a list of all documents' names\n",
    "    doc_names_xml.append(doc_names_year)\n",
    "   \n",
    "\n",
    "docnames = doc_names_pdf + doc_names_xml\n",
    "filenames_xml = get_conference_names(filenames_xml)\n",
    "filenames = filenames_pdf + filenames_xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML-Files: \n",
    "\n",
    "The XML-files are not only used for text extraction, but since they contain a lot of information due to the extensive markup, some other information will be extracted from the files in the following steps:\n",
    "- Text \n",
    "- Authors of the documents\n",
    "- Keywords given in the metadata of the abstracts in order to find the scientific methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xml_texts = []\n",
    "\n",
    "# importing the list provided, which contains all selectable options for <keywords n='keywords'>\n",
    "conf_tool_methods = open_list('Misc/conf_tool_methods.txt')\n",
    "\n",
    "# contains a list per year, this list contains a list of keywords extracted per text\n",
    "all_free_keywords = []\n",
    "used_keywords_free = []\n",
    "used_keywords_conf = []\n",
    "authors = []\n",
    "authors_full_list = []\n",
    "\n",
    "for year in all_xml_files:\n",
    "    keywords_free_year = []\n",
    "    keywords_conf_year = []\n",
    "    authors_year = []\n",
    "    for doc in year:\n",
    "        \n",
    "        soup = BeautifulSoup(doc, 'xml')\n",
    "        \n",
    "        # Code for extracting the actual text from xml-files\n",
    "        xml_text = extract_xml_text(soup)\n",
    "        all_xml_texts.append(xml_text)\n",
    "        \n",
    "        lang = detect_language(str(xml_text))\n",
    "        if lang == 'en':\n",
    "            en_count[k] += 1\n",
    "        \n",
    "        \n",
    "        # Code for extracting the author names       \n",
    "        title_stmt = soup.titleStmt\n",
    "        authors_in_doc = extract_authors(title_stmt)\n",
    "        authors_year.append(authors_in_doc) \n",
    "\n",
    "        \n",
    "        # Code for extracting the keywords used in xml-files  (per year)\n",
    "        keywords_free, keywords_conf = extract_keywords(soup, conf_tool_methods)  \n",
    "        keywords_free_year = keywords_free_year + keywords_free\n",
    "        keywords_conf_year = keywords_conf_year + keywords_conf\n",
    "        \n",
    "        \n",
    "    # saving all keywords that were given in the <keyword n=keyword> tags in the XML-files\n",
    "    all_free_keywords = list(dict.fromkeys(all_free_keywords + keywords_free_year))\n",
    "    used_keywords_conf.append(keywords_conf_year)    \n",
    "    used_keywords_free.append(keywords_free_year)     \n",
    "    \n",
    "    # saves each text's authors in a list, sorted by year of the text\n",
    "    authors.append(authors_year)\n",
    "    \n",
    "    \n",
    "    for element in authors_year:\n",
    "        authors_full_list = authors_full_list + element\n",
    "    authors_full_list = list(dict.fromkeys(authors_full_list))\n",
    "    k += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the extracted PDF and XML texts for further processing of the textual content:\n",
    "\n",
    "- Cleaning up\n",
    "- Removing stopwords depending on the detected language (English or German)\n",
    "- Lemmatizing the texts depending on the detected language (English or German) --> time-consuming step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['die', 'begutachtung', 'von', 'forschungsbeiträgen', 'ist', 'ein', 'zentraler', 'pfeiler', 'wissenschaftlicher', 'qualitätssicherung', 'sei', 'für', 'zeitschriften', 'konferenzen', 'oder', 'drittmittelfinanzierte', 'forschungsprojekte', 'dafür', 'wie', 'diese', 'begutachtung', 'konkret', 'abläuft', 'gibt', 'unterschiedliche', 'modelle', 'gepflogenheiten', 'erfahrungen', 'und', 'erwartungen', 'das', 'bei', 'dhd', 'konferenzen', 'bis', 'inklusive', 'verwendete', 'modell', 'sah', 'eine', 'teilanonymisierung', 'vor', 'autorinnen', 'waren', 'den', 'gutachterinnen', 'namentlich', 'bekannt', 'jedoch', 'nicht', 'umgekehrt', 'sog', 'single', 'blind', 'modell', 'die', 'gutachten', 'selbst', 'text', 'und', 'zahlenmäßige', 'bewertung', 'wurden', 'nur', 'den', 'autorinnen', 'allen', 'gutachterinnen', 'des', 'beitrags', 'und', 'dem', 'programmkomitee', 'mitgeteilt', 'dieses', 'modell', 'war', 'gegenstand', 'von', 'diskussionen', 'auf', 'den', 'dhd', 'mitgliederversammlungen', 'und', 'bis', 'beschlossen', 'wurde', 'für', 'die', 'nächste', 'dhd', 'konferenz', 'ein', 'zero', 'blind', 'modell', 'erproben', 'mit', 'der', 'einschränkung', 'dass', 'die', 'namen', 'der', 'gutachterinnen', 'nur', 'den', 'autorinnen', 'des', 'beitrags', 'und', 'dem', 'programmkomitee', 'bekannt', 'sind', 'aber', 'nicht', 'insgesamt', 'veröffentlicht', 'werden', 'wir', 'möchten', 'mit', 'diesem', 'panel', 'der', 'dhd', 'community', 'die', 'möglichkeit', 'geben', 'sich', 'über', 'das', 'begutachtungsverfahren', 'der', 'dhd', 'jahrestagungen', 'informieren', 'und', 'auszutauschen', 'zum', 'einen', 'ist', 'das', 'zeitkorsett', 'einer', 'mitgliederversammlung', 'eng', 'begrenzt', 'was', 'eine', 'wirkliche', 'diskussion', 'schwierig', 'macht', 'zum', 'anderen', 'betrifft', 'das', 'begutachtungsverfahren', 'auch', 'zahlreiche', 'nicht', 'mitglieder', 'für', 'eine', 'zielführende', 'und', 'produktive', 'diskussion', 'sollen', 'auf', 'dem', 'panel', 'zunächst', 'einige', 'zentrale', 'begriffe', 'geklärt', 'werden', 'die', 'der', 'diskussion', 'häufig', 'vorkommen', 'unter', 'dem', 'begriff', 'open', 'peer', 'review', 'wird', 'ein', 'bündel', 'unterschiedlicher', 'konkreter', 'verfahren', 'zusammengefasst', 'die', 'unterschiedliche', 'aspekte', 'des', 'reviewprozesses', 'öffnen', 'mit', 'open', 'reports', 'oder', 'open', 'reviews', 'ist', 'gemeint', 'dass', 'die', 'einem', 'beitrag', 'geschriebenen', 'gutachten', 'öffentlich', 'einsehbar', 'sind', 'also', 'über', 'den', 'kreis', 'der', 'autorinnen', 'des', 'beitrages', 'hinaus', 'diese', 'als', 'eigenständige', 'publikationen', 'adressier', 'und', 'zitierbar', 'sind', 'oder', 'dem', 'beitrag', 'quasi', 'als', 'eine', 'art', 'anhang', 'beigefügt', 'werden', 'ist', 'nicht', 'festgelegt', 'wirft', 'aber', 'neue', 'fragen', 'auf', 'können', 'die', 'gutachten', 'als', 'eigenständige', 'publikation', 'anonym', 'bleiben', 'können', 'sie', 'dem', 'beitrag', 'als', 'anhang', 'angefügt', 'werden', 'sodass', 'der', 'beitrag', 'immer', 'mit', 'den', 'gutachten', 'gelesen', 'werden', 'wird', 'und', 'konkreten', 'verfahren', 'wenn', 'die', 'beiträge', 'nach', 'kenntnisnahme', 'der', 'gutachten', 'von', 'den', 'autorinnen', 'überarbeitet', 'werden', 'müssen', 'dann', 'auch', 'die', 'gutachten', 'nochmal', 'überarbeitet', 'werden', 'trotz', 'dieser', 'fragen', 'liegen', 'vorteile', 'offener', 'reviews', 'klar', 'auf', 'der', 'hand', 'spätere', 'leserinnen', 'der', 'begutachteten', 'beiträge', 'können', 'kritische', 'oder', 'umstrittene', 'punkte', 'direkt', 'identifizieren', 'und', 'nicht', 'zuletzt', 'können', 'offene', 'reviews', 'auch', 'als', 'beispiele', 'für', 'erst', 'gutachterinnen', 'dienen', 'wie', 'reviews', 'aussehen', 'können', 'bei', 'open', 'identities', 'handelt', 'sich', 'den', 'kontroversesten', 'diskutierten', 'aspekt', 'grundsätzlich', 'können', 'analog', 'zur', 'verwendungsweise', 'den', 'experimentellen', 'wissenschaften', 'verschiedene', 'stufen', 'von', 'blindness', 'unterschieden', 'werden', 'beim', 'double', 'blind', 'verfahren', 'abb', 'sind', 'sowohl', 'autorinnen', 'als', 'auch', 'gutachterinnen', 'gegenseitig', 'anonym', 'aus', 'praktischen', 'gründen', 'etwa', 'interessenkonflikte', 'bei', 'der', 'zuordnung', 'verhindern', 'muss', 'eine', 'instanz', 'existieren', 'die', 'autorinnen', 'und', 'gutachterinnen', 'namentlich', 'kennt', 'das', 'programm', 'oder', 'organisationskomitee', 'bei', 'großen', 'konferenzen', 'auch', 'sog', 'area', 'chairs', 'die', 'für', 'einen', 'thematischen', 'abgegrenzten', 'bereich', 'zuständig', 'sind', 'das', 'double', 'blind', 'verfahren', 'wird', 'bei', 'den', 'konferenzen', 'der', 'association', 'for', 'computational', 'linguistics', 'acl', 'verwendet', 'wobei', 'die', 'bekanntheit', 'zwischen', 'den', 'gutachterinnen', 'unterschiedlich', 'gehandhabt', 'wird', 'abb', 'double', 'blind', 'verfahren', 'okpk', 'organisations', 'bzw', 'programmkomitee', 'roter', 'pfeil', 'name', 'nicht', 'bekannt', 'grüner', 'pfeil', 'name', 'bekannt', 'die', 'abbildung', 'bezieht', 'sich', 'auf', 'jeweils', 'einen', 'beitrag', 'der', 'pfeil', 'zwischen', 'den', 'gutachterinnen', 'repräsentiert', 'die', 'bekanntheit', 'zwischen', 'den', 'gutachterinnen', 'die', 'für', 'den', 'gleichen', 'beitrag', 'zur', 'begutachtung', 'eingeteilt', 'wurden', 'ein', 'sog', 'single', 'blind', 'verfahren', 'wurde', 'seit', 'anbeginn', 'für', 'die', 'dhd', 'konferenzen', 'angewendet', 'dabei', 'kennen', 'die', 'gutachterinnen', 'eines', 'beitrages', 'dessen', 'autorinnen', 'aber', 'nicht', 'umgekehrt', 'abb', 'zero', 'blind', 'verfahren', 'sind', 'sich', 'sowohl', 'gutachterinnen', 'als', 'auch', 'autorinnen', 'gegenseitig', 'bekannt', 'abb', 'dieses', 'verfahren', 'wird', 'bei', 'der', 'dhd', 'erstmals', 'und', 'testweise', 'angewendet', 'unterschiedlich', 'gehandhabt', 'wird', 'für', 'wen', 'genau', 'die', 'gutachterinnen', 'namentlich', 'bekannt', 'sind', 'bei', 'der', 'dhd', 'werden', 'die', 'gutachterinnen', 'namentlich', 'nur', 'den', 'jeweiligen', 'autorinnen', 'und', 'dem', 'programmkomitee', 'bekannt', 'gemacht', 'bei', 'den', 'meisten', 'begutachtungsverfahren', 'ist', 'eine', 'instanz', 'vorgesehen', 'die', 'gutachterinnen', 'beiträge', 'zuweist', 'basierend', 'auf', 'kompetenz', 'interesse', 'undoder', 'der', 'vermeidung', 'von', 'interessenkonflikten', 'bei', 'open', 'participation', 'entscheiden', 'gutachterinnen', 'selbst', 'sie', 'einen', 'beitrag', 'begutachten', 'oder', 'nicht', 'wobei', 'die', 'kenntnis', 'über', 'mögliche', 'beiträge', 'vorausgesetzt', 'wird', 'die', 'beiträge', 'müssen', 'den', 'potentiellen', 'gutachterinnen', 'zugänglich', 'sein', 'der', 'praxis', 'zieht', 'dieses', 'verfahren', 'weitere', 'fragen', 'nach', 'sich', 'wie', 'wird', 'sichergestellt', 'dass', 'alle', 'beiträge', 'begutachtet', 'werden', 'und', 'auch', 'gleich', 'viele', 'gutachten', 'erhalten', 'was', 'ist', 'die', 'motivation', 'von', 'gutachterinnen', 'beiträge', 'begutachten', 'und', 'wird', 'dadurch', 'eine', 'neue', 'schieflage', 'eingeführt', 'wir', 'werden', 'uns', 'folgenden', 'auf', 'den', 'aspekt', 'der', 'identities', 'und', 'hier', 'auf', 'die', 'optionen', 'single', 'und', 'zero', 'blind', 'konzentrieren', 'sie', 'für', 'die', 'dhd', 'konferenzen', 'mittelpunkt', 'der', 'diskussion', 'stehen', 'und', 'typischerweise', 'die', 'größten', 'kontroversen', 'auslösen', 'vgl', 'ross', 'hellauergörögh', 'panel', 'allerdings', 'können', 'auch', 'andere', 'aspekte', 'verlauf', 'der', 'diskussion', 'aufgegriffen', 'werden', 'verschiedenen', 'disziplinen', 'wurde', 'und', 'wird', 'mit', 'unterschiedlichen', 'begutachtungsverfahren', 'experimentiert', 'die', 'erfahrungen', 'dabei', 'sind', 'unterschiedlich', 'und', 'oft', 'wird', 'darüber', 'vor', 'allem', 'anekdotisch', 'oder', 'semi', 'öffentlichen', 'kreisen', 'berichtet', 'wie', 'von', 'der', 'ein', 'grund', 'dafür', 'ist', 'sicher', 'dass', 'schwer', 'ist', 'belastbare', 'vergleichende', 'studien', 'zum', 'thema', 'durchzuführen', 'der', 'review', 'prozess', 'als', 'sensibel', 'gilt', 'und', 'eine', 'große', 'zahl', 'faktoren', 'die', 'ergebnisse', 'beeinflussen', 'ein', 'solches', 'experiment', 'das', 'sich', 'nicht', 'mit', 'open', 'blind', 'beschäftigt', 'hat', 'sondern', 'mit', 'der', 'konsistenz', 'von', 'begutachtung', 'allgemeinen', 'ist', 'das', 'sog', 'neurips', 'experiment', 'corteslawrence', 'dabei', 'wurden', 'oder', 'der', 'einreichungen', 'bei', 'der', 'machine', 'learning', 'konferenz', 'neurips', 'zwei', 'unabhängigen', 'programmkommittees', 'zugewiesen', 'die', 'selbstständig', 'ihre', 'entscheidung', 'getroffen', 'haben', 'angenommen', 'wurden', 'beiträge', 'wenn', 'sie', 'von', 'mind', 'einem', 'komitee', 'angenommen', 'wurden', 'bei', 'einem', 'viertel', 'der', 'beiträge', 'kamen', 'die', 'kommitees', 'unterschiedlichen', 'entscheidungen', 'auch', 'wenn', 'dieses', 'experiment', 'sich', 'nicht', 'mit', 'open', 'blind', 'beschäftigt', 'zeigt', 'vielleicht', 'einen', 'weg', 'auf', 'wie', 'generell', 'eine', 'qualitätssicherung', 'des', 'reviewprozesses', 'aussehen', 'könnte', 'bedenken', 'ist', 'allerdings', 'dass', 'eine', 'übertragung', 'von', 'verfahren', 'aus', 'anderen', 'disziplinen', 'schon', 'deswegen', 'schwierig', 'ist', 'weil', 'diese', 'sich', 'größe', 'diversität', 'kompetitivität', 'und', 'publikationspraxis', 'massiv', 'unterscheiden', 'eine', 'einseitige', 'anonymität', 'der', 'gutachterinnen', 'wie', 'sie', 'bislang', 'bei', 'den', 'dhd', 'jahrestagungen', 'praktiziert', 'wurde', 'bringt', 'nachteile', 'für', 'die', 'autorinnen', 'mit', 'sich', 'ein', 'den', 'mitgliederversammlungen', 'und', 'benannter', 'kritikpunkt', 'ist', 'die', 'abgabe', 'von', 'ein', 'satz', 'gutachten', 'ohne', 'ernsthafte', 'auseinandersetzung', 'mit', 'den', 'eingereichten', 'beiträgen', 'geschweige', 'denn', 'konstruktiven', 'verbesserungsvorschlägen', 'für', 'die', 'autorinnen', 'gegensatz', 'dazu', 'steigen', 'umfang', 'und', 'konstruktivität', 'der', 'gutachten', 'offenen', 'begutachtungsprozessen', 'nachweislich', 'die', 'verbindlichkeit', 'und', 'rechenschaftspflicht', 'der', 'bewertungen', 'von', 'gutachterinnen', 'durch', 'die', 'offenlegung', 'ihrer', 'identität', 'deutlich', 'zunimmt', 'besançonrönnberglöwgren', 'und', 'puckerschilbertschumacher', 'eine', 'mildere', 'bewertung', 'der', 'beiträge', 'ging', 'damit', 'bei', 'der', 'zumindest', 'nicht', 'einher', 'guilianoestill', 'interner', 'bericht', 'des', 'weiteren', 'konnte', 'auch', 'kein', 'rückgang', 'der', 'bereitschaft', 'zur', 'begutachtung', 'bei', 'der', 'festgestellt', 'werden', 'nur', 'fünf', 'der', 'vorjährigen', 'gutacherinnen', 'verweigerten', 'die', 'begutachtung', 'mit', 'hinweis', 'auf', 'das', 'offene', 'verfahren', 'während', 'die', 'gesamtzahl', 'der', 'gutachterinnen', 'von', 'auf', 'anstieg', 'befürworterinnen', 'eines', 'anonymen', 'begutachtungsverfahrens', 'argumentieren', 'häufig', 'damit', 'dass', 'sich', 'autorinnen', 'durch', 'die', 'offenlegung', 'ihrer', 'forschung', 'sowie', 'die', 'reviewerinnen', 'durch', 'die', 'formulierung', 'ihrer', 'reviews', 'angreifbar', 'machen', 'und', 'eine', 'anonymisierung', 'des', 'reviewverfahrens', 'als', 'schutzmechanismus', 'fungiere', 'gerade', 'einer', 'verhältnismäßig', 'überschaubaren', 'fachgemeinschaft', 'wie', 'den', 'digital', 'humanities', 'kann', 'diese', 'anonymität', 'jedoch', 'nur', 'geringfügig', 'sichergestellt', 'werden', 'wodurch', 'eine', 'tatsächliche', 'anonymität', 'der', 'autorinnen', 'fragwürdig', 'ist', 'oder', 'nur', 'durch', 'umständlichen', 'anonymisierungsaufwand', 'den', 'beiträgen', 'gewährleistet', 'werden', 'gerade', 'bei', 'einer', 'fachlich', 'weit', 'gestreuten', 'konferenz', 'wie', 'der', 'dhd', 'reihe', 'und', 'einem', 'ebenso', 'breit', 'gefächerten', 'pool', 'gutachtenden', 'dürfte', 'die', 'offenlegung', 'der', 'identitäten', 'und', 'damit', 'der', 'fachlichen', 'expertisen', 'dazu', 'beitragen', 'dass', 'gutachterinnen', 'nur', 'beiträge', 'begutachten', 'denen', 'eine', 'fachliche', 'nähe', 'gegeben', 'ist', 'puckerschilbertschumacher', 'was', 'nicht', 'nur', 'konstruktive', 'rückmeldungen', 'begünstigt', 'sondern', 'auch', 'negative', 'gutachten', 'aufgrund', 'fachlicher', 'unkenntnis', 'verhindert', 'nicht', 'zuletzt', 'kann', 'die', 'offene', 'kommunikation', 'zwischen', 'autorinnen', 'und', 'gutachterinnen', 'nicht', 'nur', 'einer', 'qualitätssteigerung', 'des', 'eingereichten', 'beitrages', 'und', 'weiterführenden', 'diskussionen', 'führen', 'sondern', 'auch', 'impulse', 'für', 'zukünftige', 'forschungs', 'und', 'publikationsvorhaben', 'sowie', 'kollaborationen', 'geben', 'besançonrönnberglöwgren', 'offene', 'identitäten', 'reviewprozess', 'ermöglichen', 'somit', 'generell', 'eine', 'transparentere', 'gestaltung', 'die', 'eine', 'bessere', 'nachvollziehbarkeit', 'der', 'annahmen', 'und', 'ablehnungen', 'von', 'beiträgen', 'zur', 'dhd', 'jahrestagung', 'gewährleistet', 'mit', 'der', 'ablehnung', 'von', 'beiträgen', 'müssen', 'naturgemäß', 'auch', 'schlechte', 'nachrichten', 'überbracht', 'werden', 'autorinnen', 'wiederum', 'reagieren', 'unterschiedlich', 'auf', 'ablehnende', 'bewertungen', 'was', 'von', 'den', 'gutachterinnen', 'schlecht', 'oder', 'gar', 'nicht', 'vorgesehen', 'werden', 'kann', 'blindness', 'erlaubt', 'den', 'gutachterinnen', 'ihre', 'gutachten', 'unabhängig', 'von', 'solchen', 'erwägungen', 'schreiben', 'prekär', 'beschäftigte', 'müssen', 'keine', 'angst', 'haben', 'beiträge', 'etablierter', 'wissenschaftlerinnen', 'negativ', 'beurteilen', 'selbst', 'wenn', 'diese', 'ggf', 'für', 'das', 'berufliche', 'fortkommen', 'für', 'forschungsanträge', 'oder', 'empfehlungsschreiben', 'relevant', 'sind', 'ist', 'damit', 'dieser', 'effekt', 'eintritt', 'auch', 'nicht', 'entscheidend', 'die', 'etablierten', 'wissenschaftlerinnen', 'solchen', 'positionen', 'sind', 'oder', 'ihre', 'macht', 'für', 'diese', 'art', 'von', 'racheaktionen', 'ausnutzen', 'würden', 'die', 'beißhemmung', 'tritt', 'ein', 'wenn', 'gutachterinnen', 'negative', 'reaktionen', 'nur', 'befürchten', 'unabhängig', 'davon', 'sie', 'eintreten', 'ein', 'möglicher', 'langfristiger', 'effekt', 'ist', 'die', 'verkleinerung', 'des', 'pools', 'gutachterinnen', 'sowie', 'dessen', 'fortschreitende', 'ent', 'diversifikation', 'professuren', 'nach', 'wie', 'vor', 'überdurchschnittlich', 'häufig', 'von', 'männern', 'besetzt', 'werden', 'selbst', 'wenn', 'indizien', 'dafür', 'sprechen', 'dass', 'offene', 'gutachterinnen', 'namen', 'besseren', 'reviews', 'führen', 'besteht', 'die', 'gefahr', 'dass', 'gutachten', 'oberflächlich', 'bleiben', 'weil', 'sich', 'gutachterinnen', 'nicht', 'mehr', 'trauen', 'subjektive', 'und', 'ggf', 'spekulative', 'aspekte', 'anzumerken', 'welche', 'die', 'qualität', 'von', 'beiträgen', 'erheblich', 'steigern', 'können', 'insgesamt', 'ist', 'vermutlich', 'eine', 'starke', 'tendenz', 'positiven', 'reviews', 'erwarten', 'was', 'die', 'unterscheidbarkeit', 'der', 'qualität', 'der', 'beiträge', 'deutlich', 'erschweren', 'würde', 'dieser', 'effekt', 'wurde', 'bereits', 'anhand', 'eines', 'kontrollierten', 'experiments', 'des', 'british', 'journal', 'psychiatry', 'empirisch', 'untermauert', 'walsh', 'reviewers', 'who', 'signed', 'were', 'more', 'likely', 'recommend', 'publication', 'wenn', 'mehr', 'beiträge', 'positiv', 'und', 'damit', 'ähnlich', 'bewertet', 'werden', 'müssen', 'mehr', 'entscheidungen', 'durch', 'das', 'programmkomitee', 'getroffen', 'werden', 'dies', 'steigert', 'nicht', 'nur', 'den', 'arbeitsaufwand', 'für', 'wenige', 'personen', 'sondern', 'ist', 'auch', 'für', 'autorinnen', 'weniger', 'transparent', 'nach', 'abschluss', 'des', 'review', 'prozesses', 'werden', 'die', 'reviews', 'vergleichend', 'analysiert', 'auf', 'die', 'argumente', 'pro', 'und', 'contra', 'open', 'identities', 'unterstützendentkräftend', 'reagieren', 'können', 'die', 'reviews', 'sowie', 'bewertungen', 'der', 'vergangenen', 'dhds', 'köln', 'und', 'paderborn', 'stehen', 'als', 'vergleichswerte', 'zur', 'verfügung', 'die', 'bewertungen', 'können', 'direkt', 'quantitativ', 'verglichen', 'werden', 'bei', 'den', 'reviews', 'kommen', 'faktoren', 'wie', 'die', 'länge', 'genannte', 'referenzen', 'oder', 'auch', 'sentiment', 'scores', 'betracht', 'die', 'aber', 'mit', 'vorsicht', 'interpretieren', 'sind', 'darüber', 'hinaus', 'soll', 'ein', 'interview', 'mit', 'der', 'ombudsstelle', 'die', 'analyse', 'einfließen', 'die', 'panelistinnen', 'werden', 'mit', 'einleitenden', 'impulsvorträgen', 'aus', 'ihrer', 'expertise', 'und', 'erfahrungen', 'die', 'ausgangslage', 'und', 'problemfelder', 'skizzieren', 'und', 'nach', 'einem', 'austausch', 'innerhalb', 'des', 'panels', 'eine', 'offene', 'diskussion', 'mit', 'dem', 'plenum', 'übergehen', 'der', 'ablauf', 'des', 'panels', 'ist', 'wie', 'folgt', 'geplant', 'gesamtdauer', 'minuten']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Do not exert if you have the variables stored!! '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Do not exert if you have the variables stored!! '''\n",
    "whole_texts = []\n",
    "whole_texts = all_pdf_texts + all_xml_texts\n",
    "\n",
    "additional_stops = open_list('Misc/additional_stopwords.txt')\n",
    "\n",
    "list_all_texts = []\n",
    "for text in whole_texts:\n",
    "    # detecting language in order to remove the stopwords and lemmatize according to language\n",
    "    lang = detect_language(str(text))   \n",
    "    text_item = clean_text(text)\n",
    "    text_item = lemmatization(text_item, lang)\n",
    "    text_item = remove_stopwords(text_item, lang, additional_stops)\n",
    "    list_all_texts.append(text_item)\n",
    "\n",
    "\n",
    "''' Do not exert if you have the variables stored!! '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many texts are in each year's corpus taken into account?\n",
    "number_pdf_docs = [len(sublist) for sublist in doc_names_pdf]\n",
    "number_xml_docs = [len(sublist) for sublist in doc_names_xml]\n",
    "number_docs = number_pdf_docs + number_xml_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the variable *list_all_texts* as txt-file in order to conduct manual post-processing of OCR-created text from pdf-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object('Variables/', 'list_all_texts.pckl', list_all_texts)\n",
    "# list_all_texts = open_variable('Variables/', 'list_all_texts.pckl')\n",
    "\n",
    "with open('Misc/list_all_texts.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(list_all_texts[:231]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the post-processed file and eventually bringing all texts together again in variable *corr_list_of_texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the post-processed file\n",
    "with open('Misc/ocr_correction.txt', 'r', encoding='utf-8') as p:\n",
    "    pdf_ocr_corr = p.read()\n",
    "corr_list_of_texts = post_processing_ocr(pdf_ocr_corr)\n",
    "corr_list_of_texts = corr_list_of_texts[:-1]\n",
    "\n",
    "corr_list_of_texts = corr_list_of_texts + list_all_texts[231:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams and trigrams, id2word and bag of words, which are necessary for the actual topic modeling algorithm (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 7), (7, 4), (8, 1), (9, 1), (10, 1), (11, 1), (13, 3), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 2), (29, 1), (30, 1), (31, 1), (32, 4), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 4), (54, 3), (55, 5), (56, 1), (57, 1), (58, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 2), (65, 2), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 2), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 3), (91, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 2), (102, 2), (103, 2), (104, 2), (105, 2), (106, 1), (107, 4), (108, 2), (109, 1), (110, 2), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 3), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 3), (124, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 2), (134, 1), (135, 2)], [(136, 1), (137, 1), (138, 1), (139, 2), (141, 1), (143, 1), (144, 1), (145, 1), (146, 1), (150, 2), (157, 1), (160, 5), (161, 2), (163, 1), (164, 1), (165, 2), (169, 1), (170, 1), (173, 2), (175, 1), (176, 3), (179, 1), (180, 2), (182, 1), (184, 3), (185, 2), (186, 2), (188, 1), (189, 1), (190, 1), (191, 6), (192, 2), (193, 1), (197, 3), (200, 4), (204, 1), (206, 1), (209, 2), (210, 5), (212, 1), (214, 1), (215, 1), (216, 1), (219, 1), (220, 2), (221, 6), (222, 6), (223, 1), (226, 1), (231, 2), (232, 4), (233, 1), (236, 5), (238, 1), (239, 1), (242, 1), (253, 1), (254, 4), (256, 2), (260, 2), (261, 2), (264, 2), (267, 5), (268, 1), (269, 1), (275, 1), (280, 2), (281, 1), (285, 1), (287, 1), (288, 1), (291, 5), (292, 1), (295, 1), (297, 1), (298, 1), (303, 1), (306, 2), (307, 2), (309, 1), (310, 2), (311, 1), (312, 3), (315, 3), (320, 1), (322, 1), (323, 1), (324, 1), (326, 1), (327, 3), (328, 1), (332, 3), (333, 1), (334, 2), (336, 2), (337, 1), (339, 2), (341, 1), (343, 10), (344, 1), (345, 10), (346, 2), (347, 1), (348, 1), (349, 6), (351, 4), (357, 2), (358, 1), (361, 1), (364, 1), (365, 1), (368, 2), (369, 1), (370, 1), (372, 1), (373, 1), (375, 1), (377, 1), (378, 1), (379, 1), (380, 1), (382, 1), (383, 1), (384, 1), (385, 1), (388, 1), (390, 3), (391, 1), (393, 2), (395, 1), (397, 1), (399, 1), (401, 1), (404, 1), (405, 1), (406, 1), (407, 2), (408, 1), (410, 2), (412, 2), (413, 1), (415, 1), (416, 1), (418, 4), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (426, 1), (427, 1), (428, 2), (429, 4), (430, 1), (431, 2), (432, 1), (435, 1), (438, 1), (439, 4), (440, 1), (444, 3), (445, 3), (448, 1), (451, 1), (452, 1), (454, 2), (457, 2), (460, 1), (463, 1), (464, 1), (465, 1), (466, 2), (469, 2), (472, 1), (475, 1), (478, 3), (479, 1), (486, 2), (488, 3), (490, 1), (491, 1), (498, 1), (499, 2), (505, 1), (506, 2), (509, 1), (512, 2), (518, 1), (521, 1)], [(2, 4), (117, 4), (163, 1), (312, 3), (362, 3), (472, 1), (495, 2), (529, 3), (530, 1), (531, 3), (532, 2), (535, 2), (536, 1), (537, 1), (539, 1), (541, 2), (543, 1), (549, 4), (555, 3), (557, 2), (561, 1), (563, 3), (564, 1), (569, 3), (576, 3), (577, 10), (578, 2), (579, 5), (581, 1), (582, 2), (583, 3), (586, 1), (587, 1), (588, 1), (589, 18), (590, 4), (594, 4), (595, 1), (596, 4), (597, 1), (598, 7), (600, 1), (601, 5), (602, 2), (603, 3), (604, 1), (612, 2), (614, 2), (616, 2), (618, 3), (619, 1), (620, 1), (621, 1), (623, 1), (624, 4), (627, 3), (629, 2), (630, 3), (631, 2), (633, 4), (635, 1), (636, 2), (637, 1)], [(210, 3), (429, 3), (431, 4), (492, 3), (500, 2), (551, 3), (559, 2), (606, 4), (640, 2), (641, 2), (644, 1), (645, 1), (646, 1), (648, 1), (650, 1), (651, 1), (653, 1), (655, 1), (657, 1), (660, 3), (662, 1), (663, 1), (664, 1), (665, 1), (666, 1), (667, 1), (669, 2), (670, 1), (671, 1), (672, 1), (673, 1), (676, 1), (677, 1), (678, 1), (680, 1), (683, 12), (684, 1), (686, 1), (687, 1), (689, 1), (690, 1), (691, 1), (692, 1), (693, 2), (694, 2), (695, 1), (696, 1), (697, 2), (698, 1), (700, 1), (701, 2), (702, 1), (703, 2), (705, 1), (706, 1), (707, 2), (708, 1), (711, 1), (712, 1), (715, 2), (716, 1), (717, 4), (719, 1), (720, 2), (721, 1), (722, 1), (723, 1), (724, 3), (725, 3), (726, 1), (728, 3), (729, 1), (731, 1), (734, 1), (735, 1), (736, 1), (739, 1), (740, 1), (741, 1), (742, 1), (745, 1), (749, 1), (750, 1), (752, 1), (754, 1), (755, 1), (756, 1), (760, 1), (761, 3), (762, 2), (763, 1), (765, 1), (766, 1), (768, 1), (769, 1), (770, 1), (771, 1), (772, 1), (773, 1), (774, 1), (775, 1), (776, 1), (777, 1), (778, 1), (779, 1), (783, 2), (784, 3), (785, 1), (788, 2), (789, 1), (790, 1), (793, 1), (795, 1), (797, 1), (798, 7), (799, 1), (800, 1), (801, 1), (802, 1), (804, 1), (805, 1), (806, 1)], [(183, 2), (200, 3), (312, 4), (389, 2), (390, 2), (409, 1), (444, 2), (446, 1), (456, 1), (470, 1), (494, 3), (533, 1), (553, 1), (627, 2), (664, 1), (727, 1), (732, 2), (742, 1), (775, 1), (780, 3), (789, 1), (807, 1), (808, 1), (809, 1), (810, 1), (811, 1), (812, 1), (813, 1), (814, 1), (815, 2), (816, 1), (818, 1), (820, 1), (821, 1), (822, 1), (823, 1), (828, 1), (831, 1), (832, 8), (833, 1), (834, 1), (835, 1), (836, 1), (837, 3), (838, 1), (839, 1), (840, 1), (841, 1), (842, 1), (844, 1), (845, 1), (846, 1), (847, 1), (848, 1), (849, 1), (850, 1), (852, 1), (854, 1), (855, 2), (856, 1), (857, 1), (862, 2), (863, 1), (865, 6), (866, 1), (867, 1), (868, 1), (869, 1), (870, 1), (871, 1), (872, 1), (874, 1), (875, 1), (878, 1), (879, 2), (880, 1), (881, 1), (882, 1), (883, 1), (884, 1), (885, 1), (886, 1), (887, 1), (888, 1), (889, 1), (890, 2), (891, 1), (892, 1), (893, 1), (894, 2), (895, 1), (897, 2), (898, 1), (899, 1), (900, 1), (901, 1), (902, 1), (903, 1), (904, 1), (905, 1), (907, 2), (908, 1), (909, 2), (910, 1), (911, 1), (912, 1), (913, 2), (914, 1), (915, 3), (917, 1), (918, 1), (919, 1), (920, 1), (921, 1), (922, 1), (923, 1), (924, 2), (925, 1), (926, 3), (927, 1), (928, 1), (929, 3), (930, 1), (931, 1), (932, 4), (933, 1), (934, 1), (935, 1), (936, 1), (937, 2), (938, 1), (939, 1), (940, 2), (941, 1), (942, 3), (943, 1), (945, 1), (946, 1), (947, 1), (948, 3), (951, 1), (952, 1), (953, 1), (954, 1), (955, 1), (956, 1), (957, 1), (959, 3), (960, 2), (962, 1), (963, 2), (964, 2), (965, 1), (966, 1), (968, 1), (969, 1), (970, 1), (972, 2), (973, 1), (975, 1)], [(76, 1), (117, 3), (224, 1), (290, 1), (315, 2), (354, 3), (387, 2), (508, 1), (513, 1), (552, 1), (555, 2), (557, 2), (559, 1), (606, 3), (638, 1), (733, 1), (759, 2), (792, 1), (800, 1), (814, 1), (840, 1), (843, 2), (862, 5), (896, 4), (900, 3), (916, 2), (929, 2), (930, 3), (937, 1), (960, 1), (976, 1), (977, 1), (978, 1), (979, 1), (980, 1), (982, 1), (983, 1), (984, 1), (986, 1), (987, 1), (988, 1), (989, 1), (990, 1), (991, 1), (992, 1), (993, 1), (994, 1), (995, 1), (996, 1), (997, 1), (999, 1), (1001, 1), (1002, 4), (1005, 1), (1006, 3), (1007, 4), (1008, 1), (1009, 1), (1010, 1), (1011, 1), (1012, 2), (1014, 2), (1016, 2), (1017, 1), (1018, 4), (1019, 1), (1020, 1), (1021, 1), (1022, 1), (1023, 1), (1024, 1), (1026, 8), (1027, 4), (1028, 2), (1029, 1), (1030, 1), (1031, 1), (1032, 2), (1033, 1), (1034, 1), (1035, 1), (1036, 1), (1037, 1), (1038, 1), (1040, 1), (1041, 1), (1042, 1), (1043, 3), (1044, 1), (1045, 1), (1046, 1), (1048, 1), (1050, 1), (1051, 3), (1052, 1), (1053, 1), (1054, 1), (1055, 1), (1056, 1), (1057, 1), (1058, 2), (1059, 1), (1060, 1), (1061, 4), (1062, 1), (1063, 2), (1064, 1), (1065, 2), (1066, 1), (1067, 1), (1068, 2), (1069, 1), (1070, 1), (1072, 1), (1073, 1), (1075, 1), (1077, 1), (1079, 1), (1080, 1), (1082, 1), (1084, 1), (1086, 1), (1087, 1), (1089, 1), (1092, 1), (1093, 2), (1094, 1), (1095, 2), (1098, 1), (1099, 1), (1101, 2), (1102, 1), (1103, 1), (1104, 1), (1105, 2), (1106, 2), (1108, 1), (1110, 2), (1112, 2), (1113, 1), (1114, 1)], [(150, 2), (160, 2), (170, 1), (174, 1), (196, 1), (198, 1), (201, 1), (209, 1), (229, 1), (280, 1), (312, 2), (351, 1), (354, 1), (392, 3), (396, 1), (469, 1), (478, 1), (512, 3), (551, 2), (626, 1), (647, 1), (664, 1), (681, 1), (725, 3), (759, 2), (766, 1), (781, 1), (800, 1), (803, 1), (819, 2), (840, 1), (841, 1), (861, 1), (900, 2), (937, 2), (949, 1), (963, 1), (964, 1), (1002, 1), (1006, 1), (1026, 1), (1027, 2), (1031, 1), (1039, 1), (1047, 1), (1051, 1), (1116, 1), (1117, 1), (1118, 1), (1119, 1), (1120, 2), (1121, 2), (1122, 1), (1123, 1), (1124, 1), (1125, 1), (1126, 1), (1127, 1), (1128, 1), (1129, 2), (1130, 1), (1131, 1), (1132, 1), (1133, 1), (1134, 1), (1135, 1), (1136, 3), (1137, 1), (1138, 1), (1139, 1), (1140, 4), (1141, 1), (1142, 1), (1143, 1), (1144, 2), (1145, 3), (1146, 1), (1147, 1), (1148, 1), (1149, 1), (1150, 2), (1151, 1), (1152, 1), (1153, 1), (1154, 1), (1155, 1), (1157, 1), (1158, 1), (1159, 1), (1160, 2), (1161, 1), (1162, 2), (1163, 2), (1164, 2)], [(124, 1), (147, 5), (159, 8), (162, 3), (165, 2), (166, 1), (178, 3), (201, 3), (229, 9), (230, 2), (259, 7), (278, 5), (299, 7), (305, 2), (312, 9), (318, 2), (335, 6), (367, 15), (389, 4), (494, 2), (505, 2), (506, 1), (510, 1), (566, 2), (574, 2), (730, 3), (762, 3), (766, 2), (789, 1), (812, 1), (864, 2), (873, 4), (962, 1), (1032, 1), (1124, 2), (1130, 3), (1146, 4), (1165, 1), (1166, 2), (1167, 1), (1168, 1), (1169, 2), (1170, 1), (1171, 1), (1172, 1), (1174, 1), (1175, 2), (1176, 3), (1177, 2), (1179, 1), (1180, 3), (1182, 1), (1183, 1), (1184, 1), (1185, 1), (1186, 1), (1188, 1), (1189, 1), (1190, 1), (1191, 1), (1194, 1), (1195, 1), (1196, 1), (1197, 1), (1198, 1), (1200, 1), (1201, 1), (1203, 1), (1204, 1), (1205, 1), (1206, 2), (1207, 1), (1208, 3), (1209, 3), (1210, 1), (1211, 2), (1212, 1), (1213, 1), (1214, 1), (1215, 1), (1216, 1), (1221, 1), (1222, 1), (1223, 2), (1225, 1), (1226, 1), (1227, 1), (1228, 1), (1229, 1), (1230, 1), (1231, 1), (1232, 1), (1234, 1), (1235, 1), (1236, 1), (1237, 1), (1238, 1), (1240, 1), (1241, 1), (1243, 1), (1245, 1), (1246, 1), (1247, 1), (1249, 1), (1250, 2), (1251, 1), (1253, 1), (1254, 1), (1255, 1), (1256, 1), (1257, 3), (1258, 1), (1259, 1), (1261, 1), (1262, 1), (1263, 1), (1264, 3), (1265, 1), (1267, 1), (1269, 1), (1270, 1), (1271, 1), (1272, 1), (1273, 1), (1275, 1), (1276, 1), (1277, 1), (1281, 1), (1282, 2), (1285, 1), (1287, 1), (1288, 1), (1290, 2), (1292, 1), (1293, 1), (1296, 1), (1297, 1), (1298, 1), (1299, 1), (1300, 1), (1301, 2), (1302, 4), (1303, 1), (1304, 1), (1305, 1), (1307, 1), (1309, 1), (1311, 1), (1313, 1), (1314, 1), (1316, 1), (1317, 1), (1318, 1), (1319, 2), (1320, 1), (1323, 2), (1324, 1), (1325, 3), (1326, 1), (1328, 1), (1329, 1), (1330, 1), (1331, 1), (1332, 1), (1333, 1), (1334, 1), (1336, 1), (1337, 1), (1338, 1), (1340, 1), (1341, 1), (1342, 1), (1343, 1), (1344, 1), (1345, 1), (1347, 2), (1348, 1), (1350, 1), (1351, 1), (1352, 2), (1353, 1), (1354, 1), (1355, 1), (1356, 2), (1357, 1), (1358, 3), (1359, 2), (1360, 1), (1361, 1), (1362, 1), (1364, 1), (1365, 1), (1367, 1), (1368, 1), (1369, 2), (1371, 1), (1372, 1), (1373, 1), (1374, 1), (1375, 1), (1377, 2), (1378, 1), (1379, 1), (1380, 1), (1381, 1), (1382, 1), (1384, 2), (1385, 1), (1386, 1), (1389, 1), (1390, 1), (1391, 1), (1392, 1), (1394, 2), (1396, 1), (1397, 1), (1403, 1), (1404, 1), (1405, 1), (1406, 3), (1407, 1), (1409, 2), (1410, 2), (1412, 1), (1413, 1), (1414, 1), (1416, 1), (1417, 2), (1418, 1), (1420, 1), (1421, 1), (1422, 1)], [(155, 1), (205, 1), (279, 1), (308, 1), (312, 1), (354, 2), (357, 1), (393, 1), (476, 1), (480, 1), (526, 2), (538, 1), (642, 1), (646, 1), (675, 1), (710, 1), (843, 2), (879, 1), (891, 1), (916, 1), (967, 1), (974, 1), (987, 1), (1083, 1), (1177, 1), (1319, 11), (1423, 2), (1424, 1), (1425, 1), (1426, 1), (1427, 1), (1428, 1), (1429, 1), (1430, 1), (1431, 1), (1432, 2), (1433, 1), (1434, 1), (1435, 1), (1436, 1), (1437, 1), (1438, 1), (1439, 1), (1440, 2), (1442, 1), (1443, 2), (1444, 1), (1445, 1), (1446, 1), (1447, 1), (1448, 1), (1450, 1), (1451, 1), (1452, 1), (1453, 2), (1454, 4), (1455, 1), (1456, 2), (1457, 1), (1458, 1), (1459, 1), (1460, 1), (1461, 1), (1462, 1), (1463, 2), (1464, 1), (1465, 1), (1466, 1), (1467, 1), (1468, 1), (1469, 1), (1470, 1), (1471, 1), (1472, 1), (1473, 1), (1474, 1), (1475, 1), (1476, 1), (1477, 1), (1478, 1), (1479, 1), (1480, 1), (1481, 1), (1482, 1), (1483, 1), (1484, 1), (1485, 2), (1486, 2), (1487, 1), (1488, 1), (1489, 1), (1490, 1), (1491, 3), (1492, 1), (1493, 1), (1494, 7), (1495, 1), (1496, 1), (1497, 1), (1498, 1), (1499, 1), (1501, 1), (1502, 1), (1503, 1), (1504, 1), (1505, 1), (1506, 1), (1507, 1)], [(3, 1), (8, 1), (10, 2), (16, 1), (35, 3), (40, 1), (44, 1), (51, 1), (52, 1), (61, 2), (74, 3), (86, 1), (88, 1), (94, 4), (100, 2), (106, 1), (129, 5), (133, 2), (703, 3), (981, 12), (1460, 5), (1490, 2), (1508, 1), (1509, 1), (1510, 1), (1511, 2), (1512, 1), (1513, 1), (1514, 2), (1515, 1), (1516, 1), (1517, 1), (1518, 1), (1519, 1), (1520, 5), (1523, 1), (1524, 2), (1525, 1), (1526, 1), (1528, 1), (1529, 1), (1530, 2), (1531, 1), (1532, 1), (1533, 1), (1534, 1), (1535, 1), (1536, 1), (1537, 1), (1538, 1), (1539, 1), (1540, 1), (1541, 1), (1542, 1), (1543, 1), (1544, 1), (1545, 1), (1546, 1), (1547, 1), (1548, 1), (1549, 3), (1551, 1), (1552, 5), (1553, 2), (1554, 1), (1555, 2), (1556, 1), (1557, 1), (1558, 1), (1559, 1), (1560, 1), (1561, 1), (1562, 1), (1563, 1), (1564, 4), (1566, 1), (1567, 1), (1568, 1), (1569, 1), (1570, 1), (1571, 2), (1572, 1), (1573, 1), (1574, 3), (1575, 1), (1576, 1), (1577, 2), (1578, 1), (1579, 1), (1580, 1), (1582, 1), (1583, 2), (1584, 4), (1585, 1), (1586, 1), (1587, 1), (1589, 1), (1590, 1), (1591, 1), (1593, 1), (1594, 1), (1595, 1), (1596, 1), (1597, 1), (1599, 5), (1600, 1), (1601, 1), (1602, 1), (1603, 1), (1604, 1), (1605, 1), (1606, 1), (1607, 1), (1608, 2), (1609, 1), (1610, 4), (1611, 1), (1612, 1), (1613, 1), (1614, 1), (1615, 2), (1616, 1), (1617, 1), (1618, 1), (1619, 1), (1620, 1), (1621, 1), (1622, 1), (1623, 1), (1624, 1), (1625, 1), (1626, 1), (1627, 2), (1628, 1), (1629, 1), (1630, 2), (1631, 1), (1632, 3), (1633, 1), (1634, 1), (1635, 1), (1636, 1), (1638, 1), (1639, 1), (1640, 1), (1641, 1), (1642, 1), (1643, 2), (1644, 2), (1646, 1), (1647, 3), (1648, 1), (1649, 1), (1650, 1), (1651, 1), (1652, 1), (1653, 1), (1654, 2), (1655, 1), (1656, 1), (1658, 1), (1659, 1), (1660, 1), (1661, 1), (1662, 4), (1663, 1), (1664, 3), (1665, 1), (1666, 2), (1668, 1), (1669, 1), (1670, 4), (1671, 1), (1674, 1), (1675, 1), (1676, 1), (1677, 1), (1678, 1), (1679, 1), (1680, 2), (1681, 1), (1682, 2), (1683, 2), (1684, 1), (1685, 1), (1686, 1), (1687, 2), (1688, 1), (1690, 1), (1691, 1), (1692, 1), (1693, 1), (1694, 1), (1695, 1), (1696, 1), (1697, 1), (1698, 2), (1699, 1), (1700, 1), (1701, 3), (1702, 1), (1704, 1), (1705, 3), (1706, 1), (1707, 1), (1708, 2), (1709, 1), (1710, 1), (1711, 1), (1712, 3), (1713, 1), (1714, 2), (1715, 1), (1716, 1), (1717, 1), (1718, 2), (1719, 3), (1720, 2), (1721, 1), (1722, 1), (1723, 1), (1724, 1), (1727, 1), (1728, 2), (1730, 1), (1731, 1), (1732, 1), (1733, 1), (1734, 1), (1735, 5), (1736, 1), (1737, 3), (1738, 1), (1739, 1), (1740, 1), (1741, 1), (1742, 1), (1743, 1), (1744, 1), (1745, 1), (1746, 1), (1747, 4), (1748, 1), (1749, 1), (1750, 1), (1751, 1), (1752, 2), (1753, 1), (1754, 1), (1755, 1), (1756, 1), (1757, 1), (1758, 1), (1759, 1), (1760, 1), (1761, 1), (1762, 1), (1763, 1), (1764, 2), (1765, 1), (1767, 1), (1768, 1), (1769, 2), (1770, 1), (1771, 1), (1772, 1)]]\n",
      "area\n",
      "[['glossarium', 'arabicum', 'proposal', 'applicant', 'contact', 'ruhr', 'bochum', 'philologie', 'universitätsstr', 'bochum', 'mail', 'scientific', 'interest', 'ancient', 'translation', 'greek', 'philosophical', 'scientific', 'work', 'syriac', 'arabic', 'torsten', 'roeder', 'contact', 'wissenschaften', 'telota', 'jägerstraße', 'mail', 'scientific', 'interest', 'musicology', 'italian', 'language', 'literature', 'century', 'greek', 'scientific', 'philosophical', 'work', 'translate', 'large', 'extent', 'arabic', 'activity', 'incorporation', 'reorganization', 'classical', 'heritage', 'new', 'civilization', 'arabic', 'spread', 'object', 'glossarium', 'arabicum', 'make', 'available', 'scholar', 'direct', 'arabic', 'translation', 'contain', 'several', 'area', 'research', 'glossarium', 'arabicum', 'host', 'ruhr', 'bochum', 'start', 'dfg', 'fund', 'continue', 'greek', 'arabic', 'philosophical', 'concept', 'linguistic', 'bridge', 'resource', 'database', 'glossarium', 'græco', 'arabicum', 'make', 'available', 'file', 'lexical', 'intend', 'open', 'mediæval', 'arabic', 'translation', 'greek', 'contain', 'image', 'filecard', 'publish', 'analytical', 'reference', 'greek', 'arabic', 'comprise', 'arabic', 'root', 'letter', 'end', 'arabic', 'alphabet', 'database', 'provide', 'search', 'facility', 'greek', 'arabic', 'root', 'author', 'title', 'source', 'possible', 'basis', 'generate', 'entry', 'greek', 'arabic', 'dictionary', 'extend', 'effectiveness', 'database', 'intend', 'link', 'perseus', 'online', 'resource', 'technical', 'aspect', 'coexistence', 'several', 'system', 'web', 'application', 'bring', 'light', 'number', 'phenomenon', 'issue', 'compete', 'encode', 'system', 'concur', 'write', 'greek', 'arabic', 'leiden', 'direction', 'character', 'different', 'individual', 'type', 'difficulty', 'occur', 'single', 'alphabet', 'environment', 'write', 'system', 'context', 'trivial', 'problem', 'arise', 'concern', 'representation', 'research', 'content', 'database', 'website', 'input', 'researcher', 'public', 'access', 'search', 'researcher', 'frontend', 'unicode', 'seem', 'universal', 'solution', 'issue', 'require', 'complicated', 'workaround', 'disadvantage', 'user'], ['disambiguierung', 'groß', 'perspektive', 'geyk', 'saupe', 'storrer', 'technisch', 'institut', 'sprache', 'literatur', 'informatik', 'akademie', 'bbaw', 'zentrum', 'sprache', 'zentrum', 'zeithistorisch', 'forschung', 'zielsetzung', 'projekthintund', 'bieten', 'neuartig', 'keite', 'authentisch', 'untersuchen', 'clarin', 'bieten', 'flexibel', 'werkzeug', ' datengewinnung', 'quantitativ', 'groß', 'strukturiert', 'auswerten', 'gewonnen', 'nachbearbeiten', 'fall', 'wortformen', 'sprachlich', 'zeichen', 'verbindung', 'form', 'inhalt', 'auswerten', 'homonyme', 'polysem', 'textwörter', 'verfügbar', 'disambiguieren', 'unterscheidung', 'homonymen', 'lesarten', 'forschungsfrage', 'disambiguieren', 'verbunden', 'aufwand', 'zeitlich', 'restriktion', 'schungsprojekt', 'dissertation', 'studentisch', 'abschlussarbeit', 'bestimmen', 'fragestellung', 'bearbeiten', 'basieren', 'linguistisch', 'recherche', 'hilfe', 'kobra', 'arbeiten', 'germanistisch', 'linguistik', 'informatik', 'aufwand', 'manuell', 'nachbearbeitung', 'senken', 'möglichkeit', 'recherche', 'verbessern', 'aufgabe', 'aktuell', 'linguistik', 'angepassen', 'fallstudie', 'erproben', 'beteiligen', 'stellen', 'riertoße', 'baumbanke', 'integrieren', 'entwickeln', 'vorhanden', 'infrastruktur', 'fallstudie', 'beziehen', 'korpusbasiert', 'lexikographie', 'diachronisch', 'sprachforschung', 'varietätenlinguistik', 'abgeleiteten', 'handeln', 'routineaufgabe', 'arbeit', 'groß', 'filtern', 'klassifizieren', 'disambiguieren', 'visualisieren', 'verschieden', 'ähnlich', 'form', 'stellen', 'erläutern', 'konkret', 'pusbasieren', 'lexikographie', 'arbeit', 'groß', 'entstehen', 'leiten', 'anforderung', 'möglich', 'automatisch', 'stellen', 'erster', 'erprobt', 'verwenden', 'erzielen', 'dritter', 'schritt', 'analytisch', 'mehrwert', 'gabenstellungen', 'forschung', 'bildung', 'forschung', 'rahmen', 'programm', 'fördern', 'beteiligen', 'folgend', 'institution', 'projektleiter', 'germanistik', 'storrer', 'informatik', 'morik', 'geyk', 'karls', 'institut', 'sprache', 'fallstudie', 'anwendungsfeld', 'korpusbasiert', 'lexikographie', 'wichtig', 'lang', 'engelblemnitzer', 'dwds', 'kernkorpus', 'geyk', 'hinblick', 'verteilung', 'enthaltenen', 'textbeständ', 'reichen', 'belletristik', 'journalistisch', 'prosa', 'dekad', 'ausgewogen', 'lexikographe', 'suchwort', 'jahrhundert', 'gewinnen', 'verschieden', 'vergleichen', 'aussage', 'sortenspezifik', 'speziell', 'wortbedeutung', 'treffen', 'system', 'ausgegeben', 'beleg', 'homonymen', 'lexeme', 'disambiguieren', 'anzahl', 'treffer', 'suchwort', 'grenze', 'halten', 'fallen', 'storrer', 'diskutieren', 'ampel', 'disambiguierung', 'vertretbarem', 'untersuchen', 'beispielwort', 'leiter', 'resultieren', 'suchen', 'dwds', 'kernkorpus', 'liste', 'belegen', 'hersehbaren', 'anteil', 'beleg', 'homonyme', 'leiter', 'leiter', 'speziellerer', 'leiter', 'trittleiter', 'tonleiter', 'enthalten', 'lexeme', 'beleg', 'zahlen', 'disambiguierung', 'fällen', 'beschrieben', 'fallstudie', 'suchen', 'automatisch', 'disambiguierung', 'suchwort', 'belegen', 'ausgeben', 'arbeiten', 'lexikographie', 'vereinfachen', 'verbessern', 'basis', 'statistisch', 'formbasiern', 'beiten', 'komponente', 'semantisch', 'disambiguierung', 'angereicheren', 'fahren', 'entwickeln', 'gelten', 'ungewöhnlich', 'neuartig', 'verwendung', 'tag', 'fördern', 'verwandte', 'arbeiten', 'vorgestellt', 'liegen', 'reichweiten', 'forschung', 'automatisch', 'disambiguierung', 'wortbedeutung', 'disambiguation', 'zahlreich', 'arbeiten', 'widmen', 'breit', 'darstellen', 'überblick', 'agirre', 'umfangreich', 'vergleichsstudie', 'prozentuell', 'veröffentlichen', 'scheinen', 'fall', 'vorgegebenen', 'lesarten', 'disambiguieren', 'möglichkeit', 'enthalten', 'warten', 'lesarten', 'entdecken', 'ausschließen', 'folgen', 'studie', 'ansatz', 'lesarten', 'ermitteln', 'induction', 'liegen', 'reihe', 'ansatz', 'wesentliche', 'basieren', 'überblick', 'lapata', 'mithilfe', 'dirichlet', 'allocation', 'blei', 'gut', 'erzielen', 'lassen', 'basieren', 'annahmen', 'verschieden', 'lesarten', 'verwenden', 'vorkommen', 'vorkommen', 'behandelnd', 'kontextfenster', 'bestimmt', 'größlegt', 'mithilfe', 'verteilung', 'kontextwörtern', 'nennen', 'ermitteln', 'lesarten', 'aufgefasst', 'einzeln', 'kontextfenster', 'lässen', 'berechnen', 'stimmen', 'vorkommen', 'behandelnd', 'bestimmt', 'lesart', 'annehmen', 'zuordnung', 'folgen', 'zen', 'nutzen', 'grundlage', 'beispielwörter', 'erlauben', 'entstehung', 'neubedeutung', 'entwicklung', 'rekonstruieren', 'erster', 'vorgestellt', 'fallstudie', 'sprachdaen', 'dwds', 'kernkorpus', 'erproben', 'weit', 'erkenntnis', 'nutzen', 'sprachdaen', 'gewinnen', 'stammen', 'beispiel', 'leiter', 'evaluieren', 'beleg', 'dwds', 'kernkorpus', 'disambiguieren', 'beleg', 'evaluation', 'verteilung', 'lesarten', 'basis', 'kontextwörter', 'bag', 'groß', 'fenster', 'umfang', 'ganz', 'leiter', 'enthaltenden', 'satz', 'ermitteln', 'evaluation', 'reinheit', 'hinblick', 'bestimmt', 'lesarten', 'messen', 'maß', 'dienen', 'normalized', 'angewandte', 'ansatz', 'einfach', 'überlegen', 'scheinen', 'erfordern', 'beschrieben', 'verfeinerung', 'veranschaulicht', 'lassen', 'ermittel', 'häufig', 'bestimmt', 'lesarten', 'zuordnen', 'politisch', 'leiter', 'ddrdritt', 'leiter', 'sikalischer', 'leiter', 'trittleiter', 'beleg', 'leiter', 'tonleiter', 'ermitteln', 'salienz', 'syntaktisch', 'verbesserung', 'kontextfenster', 'testen', 'weit', 'dependenz', 'integrieren', 'ausführlich', 'beschreibung', 'auswertung', 'getesteten', 'ansatz', 'präsentieren', 'disziplin', 'beispiel', 'semantik', 'induktion', 'lesarten', 'grundlage', 'kontextwörtern', 'stellen', 'interessant', 'bbaw', 'kontext', 'clarin', 'zusammenarbeiten', 'semantik', 'ansiedeln', 'kollmeiersaupe', 'erscheinen', 'wandel', 'begreifen', 'verbunden', 'diskursen', 'beschäftigen', 'quantitativ', 'aufschlüsselung', 'polysemer', 'begriffe', 'gewinn', 'ermöglichen', 'schneller', 'zugriff', 'verschieden', 'bedeutung', 'rel', 'vanen', 'begreifen', 'erleichtern', 'qualitativ', 'auswertung', 'rahmen', 'kooperation', 'bbaw', 'beschrieben', 'weit', 'anwenden', 'basis', 'digitalisieren', 'zeitungskorpus', 'neu', 'berliner', 'zeitung', 'neu', 'sprachlich', 'ambiguität', 'zentral', 'begreifen', 'suchen', 'beispiel', 'einheit', 'verdeutlichen', 'stehen', 'schwindend', 'maß', 'einheit', 'einheit', 'einheit', 'wirtschafts', 'sozialpolitik', 'tauchen', 'biff', 'maßeinheit', 'fahren', 'helfen', 'semantisch', 'einheit', 'quantitativ', 'weise', 'zalysieren', 'literatur', 'agirre', 'semeval', 'blei', 'dirichlet', 'allocation', 'lapata', 'induction', 'association', 'eacl', 'strouds', 'geyk', 'dwds', 'profil', 'problems', 'extraction', 'report', 'internet', 'lexicography', 'institut', 'sprache', 'publizieren', 'arbeiten', 'linguistik', 'engelb', 'einführung', 'lexikographie', 'stauffenburg', 'geyk', 'dwds', 'corpus', 'corpus', 'language', 'fellbaum', 'corpus', 'kollmeier', 'saupe', 'erscheinen', 'ausgangspunkt', 'semantik', 'schen', 'jahrhundert', 'warnke', 'diskurs', 'zugängen', 'gnständ', 'perspektive', 'diskursmuster', 'patterns', 'beatrix', 'akademie', 'verlag', 'kollmeier', 'roundtable', 'discussion', 'geschichtlich', 'grundbegriffe', 'kollmeier', 'zeitgeschichte', 'begriffe', 'perspektive', 'schen', 'semantik', 'debatte', 'zeithistorisch', 'forschung', 'schützen', 'introduction', 'retrieval', 'corpus', 'language', 'routl', 'routl', 'disambiguation', 'squares', 'quantization', 'ieee', 'theory', 'corpus', 'bände', 'gruyter', 'association', 'oregon', 'storrer', 'korpusgestützen', 'sprachanalyse', 'lexikographie', 'phraseologie', 'angewandte', 'linguistik', 'lehrbuch'], ['kostümsprache', 'mustersprachen', 'analytisch', 'formal', 'sprechen', 'muster', 'barze', 'institut', 'architektur', 'iaas', 'barze', 'stuttgartde', 'formal', 'sprechen', 'frage', 'kostümsprache', 'film', 'problem', 'definition', 'erweisen', 'konzept', 'formal', 'sprache', 'informatik', 'nutzen', 'definition', 'geben', 'betrachten', 'bestandteil', 'kleidung', 'hose', 'hemd', 'genres', 'alphabet', 'möglich', 'kombination', 'alphabet', 'kleidung', 'genre', 'auftreten', 'möglich', 'einschränken', 'kombination', 'genre', 'auftreten', 'filtern', 'ansatz', 'geschehen', 'angeben', 'sinnvoll', 'kleidung', 'zusammengesetzt', 'anzug', 'bestehen', 'hose', 'weste', 'sakko', 'zeichen', 'anzug', 'hose', 'weste', 'sakko', 'alphabet', 'name', 'zusammengesetzt', 'kleidung', 'anzug', 'vokab', 'grammatik', 'grammatik', 'erze', 'sprache', 'genres', 'definition', 'grammatik', 'kleidung', 'genres', 'xpxsx', 'alphabet', 'vokab', 'startsymbol', 'wörtern', 'vokab', 'gelten', 'schreiben', 'iterativ', 'anwendung', 'grammatik', 'nennen', 'genres', 'bestandteil', 'kleidung', 'genres', 'sinnvoll', 'genre', 'auftretend', 'kleidung', 'genau', 'repräsentativ', 'genres', 'ableiten', 'system', 'entwickeln', 'auftretend', 'kombination', 'kleidung', 'erfassen', 'kostümsprache', 'mustersprache', 'kleidung', 'kostüm', 'kostüm', 'kleidung', 'intendiert', 'wirkung', 'festlegung', 'wirkung', 'kleidung', 'ansatz', 'repräsentieren', 'realisierung', 'funktion', 'praxis', 'geschehen', 'beispiel', 'schwellenwert', 'häufigkeit', 'auftreten', 'kleidung', 'festlegen', 'kleidung', 'schwellenwert', 'überschreiten', 'kostüm', 'auszeichnen', 'experte', 'beurteilen', 'wirkung', 'definieren', 'definition', 'heißen', 'kostümsprache', 'genres', 'kostüm', 'bewährt', 'lösung', 'wiederkehrend', 'aufgefasst', 'muster', 'sinn', 'software', 'architekturen', 'verweisen', 'muster', 'lösung', 'problems', 'verfeinerung', 'komposition', 'beschreiben', 'sprechen', 'informatik', 'mustersprachen', 'kostüme', 'verweisen', 'verschieden', 'bedeutung', 'gemeinsam', 'erscheinen', 'beschreiben', 'kostümen', 'hinweisen', 'kostümsprache', 'mustersprachen', 'techniken', 'informatik', 'umgang', 'muster', 'lassen', 'kostüme', 'übertragen', 'kostümsprache', 'system', 'erfassung', 'kleidung', 'basieren', 'erstellt', 'umfangreich', 'taxonomien', 'eigenschaft', 'taxonomien', 'kleidung', 'filmen', 'beobachter', 'einpflegen', 'geben', 'taxonomien', 'weit', 'taxonomie', 'farbnuance', 'erlauben', 'taxonomie taxonomien', 'kontrollieren', 'gegebenheit', 'anzse', 'wesentliche', 'eigenschaft', 'erfassen', 'beobachten', 'kombination', 'kleidung', 'genres', 'ende', 'erfassung', 'grammatik', 'erstellen', 'genres', 'erfassen', 'system', 'erfasst', 'kleidung', 'unterstützen', 'kostüme', 'identifizieren', 'wesentliche', 'variant', 'unterstützen', 'kostüme', 'hinweisen', 'experte', 'kleidung', 'beghen', 'wirkung', 'bestätigen ende', 'kostümsprache', 'genres', 'erstellen', 'beiteraen', 'experte', 'bewertung', 'unterstützen', 'weit', 'arbeiten', 'steuer', 'helfen', 'etabliert', 'kostümbildnerin', 'relevanten', 'taxonomien', 'anforderung', 'mächtigkeit', 'anfrage', 'system', 'autor', 'mustersprache', 'achten', 'möglich', 'nutzung', 'weit', 'muster', 'domäne']]\n",
      "[['glossarium', 'arabicum', 'proposal', 'applicant', 'contact', 'ruhr', 'bochum', 'philologie', 'universitätsstr', 'bochum', 'mail', 'scientific', 'interest', 'ancient', 'translation', 'greek', 'philosophical', 'scientific', 'work', 'syriac', 'arabic', 'torsten_roeder', 'contact', 'wissenschaften', 'telota', 'jägerstraße', 'mail', 'scientific', 'interest', 'musicology', 'italian', 'language', 'literature', 'century', 'greek', 'scientific', 'philosophical', 'work', 'translate', 'large', 'extent', 'arabic', 'activity', 'incorporation', 'reorganization', 'classical', 'heritage', 'new', 'civilization', 'arabic', 'spread', 'object', 'glossarium', 'arabicum', 'make_available', 'scholar', 'direct', 'arabic', 'translation', 'contain', 'several', 'area', 'research', 'glossarium', 'arabicum', 'host', 'ruhr', 'bochum', 'start', 'dfg', 'fund', 'continue', 'greek_arabic', 'philosophical', 'concept', 'linguistic', 'bridge', 'resource', 'database', 'glossarium', 'græco', 'arabicum', 'make_available', 'file', 'lexical', 'intend', 'open', 'mediæval', 'arabic', 'translation', 'greek', 'contain', 'image', 'filecard', 'publish', 'analytical', 'reference', 'greek_arabic', 'comprise', 'arabic', 'root', 'letter', 'end', 'arabic', 'alphabet', 'database', 'provide', 'search', 'facility', 'greek_arabic', 'root', 'author', 'title', 'source', 'possible', 'basis', 'generate', 'entry', 'greek_arabic', 'dictionary', 'extend', 'effectiveness', 'database', 'intend', 'link', 'perseus', 'online', 'resource', 'technical', 'aspect', 'coexistence', 'several', 'system', 'web_application', 'bring', 'light', 'number', 'phenomenon', 'issue', 'compete', 'encode', 'system', 'concur', 'write', 'greek_arabic', 'leiden', 'direction', 'character', 'different', 'individual', 'type', 'difficulty', 'occur', 'single', 'alphabet', 'environment', 'write', 'system', 'context', 'trivial', 'problem', 'arise', 'concern', 'representation', 'research', 'content', 'database', 'website', 'input', 'researcher', 'public', 'access', 'search', 'researcher', 'frontend', 'unicode', 'seem', 'universal', 'solution', 'issue', 'require', 'complicated', 'workaround', 'disadvantage', 'user'], ['disambiguierung', 'groß', 'perspektive', 'geyk', 'saupe', 'storrer', 'technisch', 'institut', 'sprache', 'literatur', 'informatik', 'akademie_bbaw', 'zentrum', 'sprache', 'zentrum', 'zeithistorisch', 'forschung', 'zielsetzung', 'projekthintund', 'bieten', 'neuartig', 'keite', 'authentisch', 'untersuchen', 'clarin', 'bieten', 'flexibel', 'werkzeug', ' datengewinnung', 'quantitativ', 'groß', 'strukturiert', 'auswerten', 'gewonnen', 'nachbearbeiten', 'fall', 'wortformen', 'sprachlich', 'zeichen', 'verbindung', 'form', 'inhalt', 'auswerten', 'homonyme', 'polysem', 'textwörter', 'verfügbar', 'disambiguieren', 'unterscheidung', 'homonymen', 'lesarten', 'forschungsfrage', 'disambiguieren', 'verbunden', 'aufwand', 'zeitlich', 'restriktion', 'schungsprojekt', 'dissertation', 'studentisch', 'abschlussarbeit', 'bestimmen', 'fragestellung', 'bearbeiten', 'basieren', 'linguistisch', 'recherche', 'hilfe', 'kobra', 'arbeiten', 'germanistisch', 'linguistik', 'informatik', 'aufwand', 'manuell', 'nachbearbeitung', 'senken', 'möglichkeit', 'recherche', 'verbessern', 'aufgabe', 'aktuell', 'linguistik', 'angepassen', 'fallstudie', 'erproben', 'beteiligen', 'stellen', 'riertoße', 'baumbanke', 'integrieren', 'entwickeln', 'vorhanden', 'infrastruktur', 'fallstudie', 'beziehen', 'korpusbasiert', 'lexikographie', 'diachronisch', 'sprachforschung', 'varietätenlinguistik', 'abgeleiteten', 'handeln', 'routineaufgabe', 'arbeit', 'groß', 'filtern', 'klassifizieren', 'disambiguieren', 'visualisieren', 'verschieden', 'ähnlich', 'form', 'stellen', 'erläutern', 'konkret', 'pusbasieren', 'lexikographie', 'arbeit', 'groß', 'entstehen', 'leiten', 'anforderung', 'möglich', 'automatisch', 'stellen', 'erster', 'erprobt', 'verwenden', 'erzielen', 'dritter', 'schritt', 'analytisch_mehrwert', 'gabenstellungen', 'forschung', 'bildung', 'forschung', 'rahmen', 'programm', 'fördern', 'beteiligen', 'folgend', 'institution', 'projektleiter', 'germanistik', 'storrer', 'informatik', 'morik', 'geyk', 'karls', 'institut', 'sprache', 'fallstudie', 'anwendungsfeld', 'korpusbasiert', 'lexikographie', 'wichtig', 'lang', 'engelblemnitzer', 'dwds_kernkorpus', 'geyk', 'hinblick', 'verteilung', 'enthaltenen', 'textbeständ', 'reichen', 'belletristik', 'journalistisch', 'prosa', 'dekad', 'ausgewogen', 'lexikographe', 'suchwort', 'jahrhundert', 'gewinnen', 'verschieden', 'vergleichen', 'aussage', 'sortenspezifik', 'speziell', 'wortbedeutung', 'treffen', 'system', 'ausgegeben', 'beleg', 'homonymen', 'lexeme', 'disambiguieren', 'anzahl', 'treffer', 'suchwort', 'grenze', 'halten', 'fallen', 'storrer', 'diskutieren', 'ampel', 'disambiguierung', 'vertretbarem', 'untersuchen', 'beispielwort', 'leiter', 'resultieren', 'suchen', 'dwds_kernkorpus', 'liste', 'belegen', 'hersehbaren', 'anteil', 'beleg', 'homonyme', 'leiter', 'leiter', 'speziellerer', 'leiter', 'trittleiter', 'tonleiter', 'enthalten', 'lexeme', 'beleg', 'zahlen', 'disambiguierung', 'fällen', 'beschrieben', 'fallstudie', 'suchen', 'automatisch', 'disambiguierung', 'suchwort', 'belegen', 'ausgeben', 'arbeiten', 'lexikographie', 'vereinfachen', 'verbessern', 'basis', 'statistisch', 'formbasiern', 'beiten', 'komponente', 'semantisch', 'disambiguierung', 'angereicheren', 'fahren', 'entwickeln', 'gelten', 'ungewöhnlich', 'neuartig', 'verwendung', 'tag', 'fördern', 'verwandte', 'arbeiten', 'vorgestellt', 'liegen', 'reichweiten', 'forschung', 'automatisch', 'disambiguierung', 'wortbedeutung', 'disambiguation', 'zahlreich', 'arbeiten', 'widmen', 'breit', 'darstellen', 'überblick', 'agirre', 'umfangreich', 'vergleichsstudie', 'prozentuell', 'veröffentlichen', 'scheinen', 'fall', 'vorgegebenen', 'lesarten', 'disambiguieren', 'möglichkeit', 'enthalten', 'warten', 'lesarten', 'entdecken', 'ausschließen', 'folgen', 'studie', 'ansatz', 'lesarten', 'ermitteln', 'induction', 'liegen', 'reihe', 'ansatz', 'wesentliche', 'basieren', 'überblick', 'lapata', 'mithilfe', 'dirichlet_allocation_blei', 'gut', 'erzielen', 'lassen', 'basieren', 'annahmen', 'verschieden', 'lesarten', 'verwenden', 'vorkommen', 'vorkommen', 'behandelnd', 'kontextfenster', 'bestimmt', 'größlegt', 'mithilfe', 'verteilung', 'kontextwörtern', 'nennen', 'ermitteln', 'lesarten', 'aufgefasst', 'einzeln', 'kontextfenster', 'lässen', 'berechnen', 'stimmen', 'vorkommen', 'behandelnd', 'bestimmt', 'lesart', 'annehmen', 'zuordnung', 'folgen', 'zen', 'nutzen', 'grundlage', 'beispielwörter', 'erlauben', 'entstehung', 'neubedeutung', 'entwicklung', 'rekonstruieren', 'erster', 'vorgestellt', 'fallstudie', 'sprachdaen', 'dwds_kernkorpus', 'erproben', 'weit', 'erkenntnis', 'nutzen', 'sprachdaen', 'gewinnen', 'stammen', 'beispiel', 'leiter', 'evaluieren', 'beleg', 'dwds_kernkorpus', 'disambiguieren', 'beleg', 'evaluation', 'verteilung', 'lesarten', 'basis', 'kontextwörter', 'bag', 'groß', 'fenster', 'umfang', 'ganz', 'leiter', 'enthaltenden', 'satz', 'ermitteln', 'evaluation', 'reinheit', 'hinblick', 'bestimmt', 'lesarten', 'messen', 'maß', 'dienen', 'normalized', 'angewandte', 'ansatz', 'einfach', 'überlegen', 'scheinen', 'erfordern', 'beschrieben', 'verfeinerung', 'veranschaulicht', 'lassen', 'ermittel', 'häufig', 'bestimmt', 'lesarten', 'zuordnen', 'politisch', 'leiter', 'ddrdritt', 'leiter', 'sikalischer', 'leiter', 'trittleiter', 'beleg', 'leiter', 'tonleiter', 'ermitteln', 'salienz', 'syntaktisch', 'verbesserung', 'kontextfenster', 'testen', 'weit', 'dependenz', 'integrieren', 'ausführlich', 'beschreibung', 'auswertung', 'getesteten', 'ansatz', 'präsentieren', 'disziplin', 'beispiel', 'semantik', 'induktion', 'lesarten', 'grundlage', 'kontextwörtern', 'stellen', 'interessant', 'bbaw', 'kontext', 'clarin', 'zusammenarbeiten', 'semantik', 'ansiedeln', 'kollmeiersaupe', 'erscheinen', 'wandel', 'begreifen', 'verbunden', 'diskursen', 'beschäftigen', 'quantitativ', 'aufschlüsselung', 'polysemer', 'begriffe', 'gewinn', 'ermöglichen', 'schneller', 'zugriff', 'verschieden', 'bedeutung', 'rel', 'vanen', 'begreifen', 'erleichtern', 'qualitativ', 'auswertung', 'rahmen', 'kooperation', 'bbaw', 'beschrieben', 'weit', 'anwenden', 'basis', 'digitalisieren', 'zeitungskorpus', 'neu', 'berliner', 'zeitung', 'neu', 'sprachlich', 'ambiguität', 'zentral', 'begreifen', 'suchen', 'beispiel', 'einheit', 'verdeutlichen', 'stehen', 'schwindend', 'maß', 'einheit', 'einheit', 'einheit', 'wirtschafts', 'sozialpolitik', 'tauchen', 'biff', 'maßeinheit', 'fahren', 'helfen', 'semantisch', 'einheit', 'quantitativ', 'weise', 'zalysieren', 'literatur', 'agirre', 'semeval', 'blei_dirichlet', 'allocation', 'lapata', 'induction', 'association', 'eacl', 'strouds', 'geyk', 'dwds', 'profil', 'problems', 'extraction', 'report', 'internet', 'lexicography', 'institut', 'sprache', 'publizieren', 'arbeiten', 'linguistik', 'engelb', 'einführung', 'lexikographie', 'stauffenburg', 'geyk', 'dwds', 'corpus', 'corpus', 'language', 'fellbaum', 'corpus', 'kollmeier', 'saupe', 'erscheinen', 'ausgangspunkt', 'semantik', 'schen', 'jahrhundert', 'warnke', 'diskurs', 'zugängen', 'gnständ', 'perspektive', 'diskursmuster', 'patterns', 'beatrix', 'akademie', 'verlag', 'kollmeier', 'roundtable', 'discussion', 'geschichtlich', 'grundbegriffe', 'kollmeier', 'zeitgeschichte', 'begriffe', 'perspektive', 'schen', 'semantik', 'debatte', 'zeithistorisch', 'forschung', 'schützen', 'introduction', 'retrieval', 'corpus', 'language', 'routl', 'routl', 'disambiguation', 'squares', 'quantization', 'ieee', 'theory', 'corpus', 'bände', 'gruyter', 'association', 'oregon', 'storrer', 'korpusgestützen', 'sprachanalyse', 'lexikographie', 'phraseologie', 'angewandte', 'linguistik', 'lehrbuch'], ['kostümsprache', 'mustersprachen', 'analytisch', 'formal', 'sprechen', 'muster', 'barze', 'institut_architektur_iaas', 'barze', 'stuttgartde', 'formal', 'sprechen', 'frage', 'kostümsprache', 'film', 'problem', 'definition', 'erweisen', 'konzept', 'formal', 'sprache', 'informatik', 'nutzen', 'definition', 'geben', 'betrachten', 'bestandteil', 'kleidung', 'hose', 'hemd', 'genres', 'alphabet', 'möglich', 'kombination', 'alphabet', 'kleidung', 'genre', 'auftreten', 'möglich', 'einschränken', 'kombination', 'genre', 'auftreten', 'filtern', 'ansatz', 'geschehen', 'angeben', 'sinnvoll', 'kleidung', 'zusammengesetzt', 'anzug', 'bestehen', 'hose', 'weste', 'sakko', 'zeichen', 'anzug', 'hose', 'weste', 'sakko', 'alphabet', 'name', 'zusammengesetzt', 'kleidung', 'anzug', 'vokab', 'grammatik', 'grammatik', 'erze', 'sprache', 'genres', 'definition', 'grammatik', 'kleidung', 'genres', 'xpxsx', 'alphabet', 'vokab', 'startsymbol', 'wörtern', 'vokab', 'gelten', 'schreiben', 'iterativ', 'anwendung', 'grammatik', 'nennen', 'genres', 'bestandteil', 'kleidung', 'genres', 'sinnvoll', 'genre', 'auftretend', 'kleidung', 'genau', 'repräsentativ', 'genres', 'ableiten', 'system', 'entwickeln', 'auftretend', 'kombination', 'kleidung', 'erfassen', 'kostümsprache', 'mustersprache', 'kleidung', 'kostüm', 'kostüm', 'kleidung', 'intendiert', 'wirkung', 'festlegung', 'wirkung', 'kleidung', 'ansatz', 'repräsentieren', 'realisierung', 'funktion', 'praxis', 'geschehen', 'beispiel', 'schwellenwert', 'häufigkeit', 'auftreten', 'kleidung', 'festlegen', 'kleidung', 'schwellenwert', 'überschreiten', 'kostüm', 'auszeichnen', 'experte', 'beurteilen', 'wirkung', 'definieren', 'definition', 'heißen', 'kostümsprache', 'genres', 'kostüm', 'bewährt', 'lösung', 'wiederkehrend', 'aufgefasst', 'muster', 'sinn', 'software', 'architekturen', 'verweisen', 'muster', 'lösung_problems', 'verfeinerung', 'komposition', 'beschreiben', 'sprechen', 'informatik', 'mustersprachen', 'kostüme', 'verweisen', 'verschieden', 'bedeutung', 'gemeinsam', 'erscheinen', 'beschreiben', 'kostümen', 'hinweisen', 'kostümsprache', 'mustersprachen', 'techniken', 'informatik', 'umgang', 'muster', 'lassen', 'kostüme', 'übertragen', 'kostümsprache', 'system', 'erfassung', 'kleidung', 'basieren', 'erstellt', 'umfangreich', 'taxonomien', 'eigenschaft', 'taxonomien', 'kleidung', 'filmen', 'beobachter', 'einpflegen', 'geben', 'taxonomien', 'weit', 'taxonomie', 'farbnuance', 'erlauben', 'taxonomie taxonomien', 'kontrollieren', 'gegebenheit', 'anzse', 'wesentliche', 'eigenschaft', 'erfassen', 'beobachten', 'kombination', 'kleidung', 'genres', 'ende', 'erfassung', 'grammatik', 'erstellen', 'genres', 'erfassen', 'system', 'erfasst', 'kleidung', 'unterstützen', 'kostüme', 'identifizieren', 'wesentliche', 'variant', 'unterstützen', 'kostüme', 'hinweisen', 'experte', 'kleidung', 'beghen', 'wirkung', 'bestätigen ende', 'kostümsprache', 'genres', 'erstellen', 'beiteraen', 'experte', 'bewertung', 'unterstützen', 'weit', 'arbeiten', 'steuer', 'helfen', 'etabliert', 'kostümbildnerin', 'relevanten', 'taxonomien', 'anforderung', 'mächtigkeit', 'anfrage', 'system', 'autor', 'mustersprache', 'achten', 'möglich', 'nutzung', 'weit', 'muster', 'domäne']]\n"
     ]
    }
   ],
   "source": [
    "# creating bigrams and trigrams from lemmatized words\n",
    "data_bigrams_trigrams = create_bigrams_trigrams(corr_list_of_texts)\n",
    "\n",
    "# id2word as dictionary where every word/bi-/trigram is referenced with id\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "\n",
    "# corpus as dictionary that contains a list of tuples for each document, tuples contain (word id, no. of appearances of the word)\n",
    "# some index numbers are missing due to the tf-idf weighting \n",
    "corpus = tf_idf(id2word, data_bigrams_trigrams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the variables for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the files to save the variables\n",
    "save_object('Variables/', 'corpus.pckl', corpus)\n",
    "save_object('Variables/', 'id2word.pckl', id2word)\n",
    "save_object('Variables/', 'data_bigrams_trigrams.pckl', data_bigrams_trigrams)\n",
    "save_object('Variables/', 'corr_list_of_texts.pckl', corr_list_of_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information/transparency purposes: Saving information on the corpus (e.g. for mentioning in the Thesis text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus statistics\n",
    "statistics = pd.DataFrame([doc_statistics, number_docs, en_count], index=[\"Total No. of Documents\", \"No. of Documents Ffter Filtering\", \"Documents in English\"], \n",
    "                   columns=['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2022', '2023'])\n",
    "statistics.to_csv('Figures/Statistics_Corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'number_pdf_docs' (list)\n",
      "Stored 'number_xml_docs' (list)\n",
      "Stored 'number_docs' (list)\n",
      "Stored 'docnames' (list)\n",
      "Stored 'filenames_xml' (list)\n",
      "Stored 'all_free_keywords' (list)\n",
      "Stored 'used_keywords_free' (list)\n",
      "Stored 'used_keywords_conf' (list)\n",
      "Stored 'authors' (list)\n",
      "Stored 'authors_full_list' (list)\n"
     ]
    }
   ],
   "source": [
    "%store number_pdf_docs\n",
    "%store number_xml_docs\n",
    "%store number_docs\n",
    "%store docnames\n",
    "%store filenames_xml\n",
    "%store all_free_keywords\n",
    "%store used_keywords_free\n",
    "%store used_keywords_conf\n",
    "%store authors\n",
    "%store authors_full_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "265bbd40db63aa34df1bd83f77ecf498882faae903508c6e893ae6addaebaa43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
