{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Master's Thesis: Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. Which topics can be found in the abstracts from DHd-conferences between 2014 and 2023 with Topic Modeling?\n",
    "\n",
    "2. Which topics appear frequently in one abstract and therefore have a high topic similarity?\n",
    "\n",
    "3. How have the topics been changing throughout the years - which trends are perceptible?\n",
    "\n",
    "4. With regard to the use of different scientific methods, which developments are perceptible?\n",
    "\n",
    "5. Which researchers contribute to the conference particularly frequently with abstracts, in which teams do they contribute and how have the teams been changing?\n",
    "\n",
    "6. Which clusters of researchers can be found with regard to topics and how have the clusters been changing?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "cell #1"
    ]
   },
   "outputs": [],
   "source": [
    "#Reading in necessary pdf- and xml-files\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz\n",
    "from io import BytesIO\n",
    "\n",
    "#(pre)processing the files\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from HanTa import HanoverTagger as ht\n",
    "from langdetect import detect\n",
    "from gensim.models import TfidfModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions: opening lists, saving and reopening objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [
     "cell #2"
    ]
   },
   "outputs": [],
   "source": [
    "def open_list(doc_name):\n",
    "    f = open(doc_name, \"r\", encoding='utf-8')\n",
    "    data = f.read()\n",
    "    data = data.split(\", \")\n",
    "    f.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "cell #3"
    ]
   },
   "outputs": [],
   "source": [
    "def save_object(dirname, filename, varname):\n",
    "    filename = dirname + filename\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(varname, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "cell #4"
    ]
   },
   "outputs": [],
   "source": [
    "def open_variable(dirname, filename):\n",
    "    path = str(dirname) + str(filename)\n",
    "    f = open(path, 'rb')\n",
    "    filename = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "cell #5"
    ]
   },
   "outputs": [],
   "source": [
    "def check_directory(directory_name):\n",
    "    if not os.path.isdir(directory_name):\n",
    "        os.mkdir(directory_name)\n",
    "        print('Created new directory: ', directory_name)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### Extracting Text from XML-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "cell #6"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_xml_text(soup):\n",
    "    \n",
    "    # extract <p> tags from body of xml-document to find the actual document text \n",
    "    p_tags = soup.body.find_all(\"p\")\n",
    "    \n",
    "    return p_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining Text Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "cell #7"
    ]
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    return detect(text)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Texts\n",
    "\n",
    "Removing new lines, weblinks, digits, markup and punctuation. Further using gensim.utils.simple_preprocess which eliminates tokens shorter than 3 or longer than 30 characters, and returns lower-cased items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "cell #8"
    ]
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \n",
    "    clean_text = re.sub(r'\\n', \"\", str(text))\n",
    "\n",
    "    sequences_to_remove = [r'http(.*?) ', r'\\d', r'<(.*?)>', r'https(.*?) ', r'www(.*?) ', '-']\n",
    "    for item in sequences_to_remove:\n",
    "        clean_text = re.sub(item, '', clean_text)\n",
    "    \n",
    "    # filtering punctuation\n",
    "    punctuation = '''!“()´`¨[]{}\\\\;:”\",<>/.?@#$%^&*_~''' \n",
    "    for item in clean_text:\n",
    "        if item in punctuation:\n",
    "            clean_text = clean_text.replace(item, \"\")\n",
    "      \n",
    "    # convert a document into a list of lowercase tokens, ignoring tokens that are too short (min_len=3) or too long (max_len=30), no deaccentation (by default)\n",
    "    clean_text = gensim.utils.simple_preprocess(clean_text, min_len=3, max_len=30)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization with Part-of-Speech Tagging\n",
    "\n",
    "Lemmatizing words tagged with certain part of speech and according to the detected language. Hanover Tagger is used since it lemmatized better than spaCy tagger, especially on German words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "cell #9"
    ]
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, language):\n",
    "    \n",
    "    tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "    tagger_en = ht.HanoverTagger('morphmodel_en.pgz')\n",
    "    \n",
    "    allowed_tags = ['NN', 'NE', 'ADJ(A)', 'ADJ(D)', 'VV(INF)', 'VV(FIN)', 'VV(PP)', 'VA(INF)', 'VA(FIN)', 'VM(FIN)', 'VM(INF)']\n",
    "\n",
    "    if language == 'de':\n",
    "        lemmatized_text = []\n",
    "        for token in texts:\n",
    "            tagged_token = tagger_de.analyze(token)\n",
    "            if tagged_token[1] in allowed_tags:\n",
    "                lemmatized_text.append(tagged_token[0].lower())\n",
    "\n",
    "    elif language == 'en':\n",
    "        lemmatized_text=[]\n",
    "        for token in texts:\n",
    "            tagged_token = tagger_en.analyze(token)\n",
    "            if tagged_token[1] in allowed_tags:\n",
    "                lemmatized_text.append(tagged_token[0].lower())\n",
    "            \n",
    "    return lemmatized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "Removing stopwords contained in standard lists for German and English, as well as from own list set up especially for the corpus used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "cell #10"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, language, additional_stops):\n",
    "    \n",
    "    stopwords_de = set(stopwords.words('german'))\n",
    "    stopwords_de.update(additional_stops)\n",
    "    \n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    stopwords_en.update(additional_stops)\n",
    "    \n",
    "    filtered_text = []\n",
    "    for w in text:\n",
    "        if language == 'de':\n",
    "            if w not in stopwords_de:\n",
    "                filtered_text.append(w)\n",
    "\n",
    "        elif language == 'en':\n",
    "            if w not in stopwords_en:\n",
    "                filtered_text.append(w)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract List Items from Textfile After Manual OCR-Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "cell #11"
    ]
   },
   "outputs": [],
   "source": [
    "def post_processing_ocr(manually_corrected_file):\n",
    "\n",
    "    postprocessed_texts = []\n",
    "    for item in manually_corrected_file.split(r']'):\n",
    "        item = re.sub(r', \\[', '', item)\n",
    "        item = re.sub(r'\\ufeff', '', item)\n",
    "        item = re.sub(r'\\[', '', item)\n",
    "        item = re.sub(r'\\'', '', item)\n",
    "        item = re.sub(r'\\’', '', item)\n",
    "        item = re.sub(r'\\‘', '', item)\n",
    "        clean = item.split(', ')\n",
    "        postprocessed_texts.append(clean)\n",
    "        \n",
    "    # cutting off the last two items as they are not text    \n",
    "    postprocessed_texts = postprocessed_texts[:-2]\n",
    "    \n",
    "    return postprocessed_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating N-Grams\n",
    "\n",
    "Creating n-grams by using classes *Phrases* and *Phraser* provided by *gensim.models*. Those classes identify phrases within the texts which qualify for n-grams, given the minimum count and the threshold. *create_bigrams* and *create_trigrams* then return the actual n-grams to the text basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "cell #12"
    ]
   },
   "outputs": [],
   "source": [
    "def create_bigrams(texts, bigram):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def create_trigrams(texts, trigram, bigram):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "cell #13"
    ]
   },
   "outputs": [],
   "source": [
    "def create_bigrams_trigrams(texts):\n",
    "   \n",
    "    bigram_phrases = gensim.models.Phrases(texts, min_count = 8, threshold = 100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[texts], threshold = 100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    data_bigrams = create_bigrams(texts, bigram)\n",
    "    data_bigrams_trigrams = create_trigrams(data_bigrams, trigram, bigram)\n",
    "\n",
    "    return data_bigrams_trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Weighting\n",
    "\n",
    "TF-IDF weighting eliminates terms which are very frequent, i.e. ubiquitous, in the documents and therefore might not be very important to the texts' meaning. The threshold determines which TF-IDF value the terms have to overcome in order to not be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "cell #14",
     "cell #15"
    ]
   },
   "outputs": [],
   "source": [
    "def tf_idf(id2word, texts):\n",
    "    \n",
    "    # simple bag of words for each document, containing tuples with (index, number of appearances of the word in the document)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # calculates term frequency (TF) weighted by the inverse document frequency (IDF) for every word/index in the bag of words\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    # low_value as threshold\n",
    "    threshold = 0.03\n",
    "    deleted_words  = []\n",
    "\n",
    "    # for every single bag of words\n",
    "    for i in range(0, len(corpus)):\n",
    "        # consider each bow for each document\n",
    "        bow = corpus[i]\n",
    "        \n",
    "        # if the value in the (id, tfidf-value) tuple is lower than 0.03, put id into list low_value_words\n",
    "        below_threshold_ids = [id for id, value in tfidf[bow] if value < threshold]\n",
    "\n",
    "        # which words will be deleted from the bow?\n",
    "        for id in below_threshold_ids:\n",
    "            deleted_words.append(id2word[id])\n",
    "        \n",
    "        # add words which indexes are not in low_value_words and not in words_missing_in_tfidf to the new bag of words \n",
    "        new_bow = [b for b in bow if b[0] not in below_threshold_ids]\n",
    "        \n",
    "        # new bow is missing certain indexes\n",
    "        corpus[i] = new_bow\n",
    "    \n",
    "    return corpus, deleted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering the Research Questions\n",
    "#### Extracting Conference Names from Zip-Files\n",
    "\n",
    "*get_conference_nams* extracts the conference name from the zip-file name so that the proper names can be used for referencing the conferences in e.g. visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "cell #15"
    ]
   },
   "outputs": [],
   "source": [
    "def get_conference_names(list):\n",
    "    \n",
    "    files = []\n",
    "    for element in list:\n",
    "        new_name = element.split('.zip')[0]\n",
    "        new_name = new_name.split('Corpus/')[1]\n",
    "        new_name = re.sub('_', ' ', new_name)\n",
    "        files.append(new_name)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Author Names\n",
    "*extract_authors* finds the names of the authors and returns a list of lists containing the names of the single texts' authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "cell #16"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_authors(title_stmt):\n",
    "    \n",
    "    # navigating to the title statement and finding all tags <author>\n",
    "    authors = title_stmt.find_all(\"author\")\n",
    "    fore_and_surnames = []\n",
    "    \n",
    "    # extracting the <surname> and <forename> tags and cleaning the outcome from the tags and the brackets\n",
    "    for element in authors:\n",
    "        names = element.find_all(['surname', 'forename'])\n",
    "        names =  re.sub(\"<(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(\"</(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(r'\\]', \"\", names)\n",
    "        names = re.sub(r'\\[', \"\", names)\n",
    "        fore_and_surnames.append(names)\n",
    "    \n",
    "    return fore_and_surnames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Keywords from XML-File\n",
    "\n",
    "- *extract_keywords*\n",
    "\n",
    "extracts tags \\<keywords n='topics' scheme='ConfTool'> and \\<keywords n='keywords' scheme='ConfTool'> to get keywords of the texts\n",
    "\n",
    "- *remove_markup_and_short_keywords*\n",
    "\n",
    "cleans keywords from xml-markup and removes those shorter than three characters\n",
    "\n",
    "- *remove_invalid_keywords*\n",
    "\n",
    "checks validity of keywords from \\<keywords n='topics'> by looking of keywords can be found in list of predetermined keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "cell #17"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_markup_and_short_keywords(keywords):\n",
    "    \n",
    "    keywords = re.sub(\"<(.*?)>\", \"\", keywords)\n",
    "    keywords = keywords.split(\"\\n\")\n",
    "    for item in keywords:\n",
    "        if len(item) <= 2:\n",
    "            keywords.remove(item)\n",
    "            \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "cell #18"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_invalid_keywords(keywords_conflist, conf_tool_methods):\n",
    "\n",
    "    for item in keywords_conflist:\n",
    "        if item not in conf_tool_methods:\n",
    "            keywords_conflist.remove(item)\n",
    "    \n",
    "    return keywords_conflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "cell #19"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_keywords(xmldata, conf_tool_methods):\n",
    "    \n",
    "    # finds all tags <keywords n=\"keywords\"> and <keywords n=\"topics\">, removes all tags within\n",
    "    keywords_freely_selectable = str(xmldata.find_all('keywords', n='keywords'))\n",
    "    keywords_conflist = str(xmldata.find_all('keywords', n='topics'))\n",
    "    \n",
    "    keywords_freely_selectable = remove_markup_and_short_keywords(keywords_freely_selectable)\n",
    "    keywords_conflist = remove_markup_and_short_keywords(keywords_conflist)\n",
    "    \n",
    "    remove_invalid_keywords(keywords_conflist, conf_tool_methods)\n",
    " \n",
    "    return keywords_freely_selectable, keywords_conflist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating folders in which variables, models and figures can be saved later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": [
     "cell #20"
    ]
   },
   "outputs": [],
   "source": [
    "check_directory('Variables/')\n",
    "check_directory('Models/')\n",
    "\n",
    "rqs = ['RQ1', 'RQ2', 'RQ3', 'RQ4', 'RQ5', 'RQ6', ]    \n",
    "for section in rqs:\n",
    "    check_directory('Figures/' + section)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in zip-files of DHd-conferences where only PDF-files are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "cell #21"
    ]
   },
   "outputs": [],
   "source": [
    "filenames_pdf = ['Corpus/DHd_2014.zip', 'Corpus/DHd_2015.zip']\n",
    "document_statistics = []\n",
    "\n",
    "# extracting text from pdf-files\n",
    "all_pdf_texts = []\n",
    "doc_names_pdf = []\n",
    "\n",
    "#creating a list containing 9 zeros, which is filled with statistical information in the process of running the code\n",
    "count_english_texts = [0] * 9\n",
    "year_index = 0\n",
    "\n",
    "for conference_file in filenames_pdf:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    document_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.pdf':\n",
    "            doc_names_year.append(name)\n",
    "            pdf_data = BytesIO(archive.read(name))\n",
    "            # reading each pdf-file in the zip-archive\n",
    "            with fitz.open(stream=pdf_data, filetype='pdf') as doc:\n",
    "                text = ''\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "\n",
    "                all_pdf_texts.append(text)\n",
    "                # detecting text language here for statistics\n",
    "                lang = detect_language(text)\n",
    "                if lang == 'en':\n",
    "                    count_english_texts[year_index] += 1\n",
    "\n",
    "\n",
    "    doc_names_pdf.append(doc_names_year)\n",
    "    year_index += 1\n",
    "    \n",
    "\n",
    "filenames_pdf = get_conference_names(filenames_pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the zip-files of the DHd-Conferences where XML-files were published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "cell #22"
    ]
   },
   "outputs": [],
   "source": [
    "filenames_xml = ['Corpus/DHd_2016.zip', 'Corpus/DHd_2017.zip', 'Corpus/DHd_2018.zip', 'Corpus/DHd_2019.zip', 'Corpus/DHd_2020.zip',\n",
    "             'Corpus/DHd_2022.zip', 'Corpus/DHd_2023.zip']\n",
    "\n",
    "all_xml_files = []\n",
    "doc_names_xml = []\n",
    "# read in all zip-folders\n",
    "for conference_file in filenames_xml:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    xml_per_year = []\n",
    "    document_statistics.append(len(archive.namelist()))\n",
    "    # read in all files in the zip-file and check that they are xml-files\n",
    "    # exclude final.xml since those are not documents about a presentation\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.xml' and not name[-9:] == 'final.xml':\n",
    "            xml_per_year.append(archive.read(name))\n",
    "            doc_names_year.append(name)\n",
    "    all_xml_files.append(xml_per_year)\n",
    "    # creating a list of all documents' names\n",
    "    doc_names_xml.append(doc_names_year)\n",
    "   \n",
    "\n",
    "docnames = doc_names_pdf + doc_names_xml\n",
    "filenames_xml = get_conference_names(filenames_xml)\n",
    "filenames = filenames_pdf + filenames_xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML-Files: \n",
    "\n",
    "The XML-files are not only used for text extraction, but since they contain a lot of information due to the extensive markup, some other information will be extracted from the files in the following steps:\n",
    "- Text \n",
    "- Authors of the documents\n",
    "- Keywords given in the metadata of the abstracts in order to find the scientific methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "cell #23"
    ]
   },
   "outputs": [],
   "source": [
    "all_xml_texts = []\n",
    "\n",
    "# importing the list provided, which contains all selectable options for <keywords n='keywords'>\n",
    "list_predetermined_keywords = open_list('Misc/predetermined_keywords.txt')\n",
    "\n",
    "# contains a list per year, this list contains a list of keywords extracted per text\n",
    "all_freely_selectable_keywords = []\n",
    "used_keywords_freely_selectable = []\n",
    "used_keywords_predetermined = []\n",
    "authors = []\n",
    "authors_full_list = []\n",
    "\n",
    "for year in all_xml_files:\n",
    "    keywords_freely_selectable_year = []\n",
    "    keywords_predetermined_year = []\n",
    "    authors_year = []\n",
    "    for doc in year:\n",
    "        \n",
    "        soup = BeautifulSoup(doc, 'xml')\n",
    "        \n",
    "        # Code for extracting the actual text from xml-files\n",
    "        xml_text = extract_xml_text(soup)\n",
    "        all_xml_texts.append(xml_text)\n",
    "        \n",
    "        lang = detect_language(str(xml_text))\n",
    "        if lang == 'en':\n",
    "            count_english_texts[year_index] += 1\n",
    "        \n",
    "        # Code for extracting the author names from titleStatement      \n",
    "        authors_in_doc = extract_authors(soup.titleStmt)\n",
    "        authors_year.append(authors_in_doc) \n",
    "\n",
    "        # Code for extracting the keywords used in xml-files  (per year)\n",
    "        keywords_freely_selectable, keywords_predetermined = extract_keywords(soup, list_predetermined_keywords)  \n",
    "        keywords_freely_selectable_year = keywords_freely_selectable_year + keywords_freely_selectable\n",
    "        keywords_predetermined_year = keywords_predetermined_year + keywords_predetermined\n",
    "        \n",
    "    # saving all keywords that were given in the <keyword n=keyword> tags in the XML-files\n",
    "    all_freely_selectable_keywords = list(dict.fromkeys(all_freely_selectable_keywords + keywords_freely_selectable_year))\n",
    "    used_keywords_predetermined.append(keywords_predetermined_year)    \n",
    "    used_keywords_freely_selectable.append(keywords_freely_selectable_year)     \n",
    "    \n",
    "    # saves each text's authors in a list, sorted by year of the text\n",
    "    authors.append(authors_year)\n",
    "    year_index += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the extracted PDF and XML texts for further processing of the textual content:\n",
    "\n",
    "- Determining text language\n",
    "- Cleaning the text from unneccessary contents\n",
    "- Lemmatizing the texts depending on the detected language (English or German) --> time-consuming step\n",
    "- Removing stopwords depending on the detected language (English or German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "cell #24"
    ]
   },
   "outputs": [],
   "source": [
    "# Time-consuming step: Do not excute if you have the variables stored!\n",
    "whole_texts = []\n",
    "whole_texts = all_pdf_texts + all_xml_texts\n",
    "\n",
    "additional_stopwords = open_list('Misc/additional_stopwords.txt')\n",
    "\n",
    "list_all_texts = []\n",
    "for text in whole_texts:\n",
    "    # detecting language in order to remove the stopwords and lemmatize according to language\n",
    "    lang = detect_language(str(text)) \n",
    "    text = clean_text(text)\n",
    "    text = lemmatization(text, lang)\n",
    "    text = remove_stopwords(text, lang, additional_stopwords)\n",
    "    list_all_texts.append(text)\n",
    "save_object('Variables/', 'list_all_texts.pckl', list_all_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a file *to_correct_ocr.txt* which contains the retrieved texts from the pdf-files in order to manually postprocess/clean possible OCR mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "cell #25"
    ]
   },
   "outputs": [],
   "source": [
    "with open('Misc/to_correct_ocr.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(list_all_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the post-processed file and eventually bringing all texts together again in variable *corr_list_of_texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "cell #26"
    ]
   },
   "outputs": [],
   "source": [
    "# opening the post-processed file\n",
    "with open('Misc/corrected_text.txt', 'r', encoding='utf-8') as f:\n",
    "    manually_corrected_file = f.read()\n",
    "\n",
    "corrected_list_of_texts = post_processing_ocr(manually_corrected_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams and trigrams, id2word and corpus, which are necessary for the actual topic modeling algorithm (LDA). For transparency and control purposes, the terms deleted by tf-idf weighting are written into a file and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "cell #27"
    ]
   },
   "outputs": [],
   "source": [
    "# creating bigrams and trigrams from lemmatized words\n",
    "data_bigrams_trigrams = create_bigrams_trigrams(corrected_list_of_texts)\n",
    "\n",
    "# id2word as dictionary where every word/bi-/trigram is referenced with id\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "# corpus as dictionary that contains a list of tuples for each document, tuples contain (word id, no. of appearances of the word)\n",
    "# some index numbers are missing due to the tf-idf weighting \n",
    "corpus, deleted_words = tf_idf(id2word, data_bigrams_trigrams)\n",
    "\n",
    "with open('Misc/tfidf_deleted_terms.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(deleted_words))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time by not having to execute time-consuming steps every time again, important variables are saved and can be reopened in the following two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "cell #28"
    ]
   },
   "outputs": [],
   "source": [
    "# writing the files to save the variables\n",
    "save_object('Variables/', 'corrected_list_of_texts.pckl', corrected_list_of_texts)\n",
    "save_object('Variables/', 'data_bigrams_trigrams.pckl', data_bigrams_trigrams)\n",
    "save_object('Variables/', 'id2word.pckl', id2word)\n",
    "save_object('Variables/', 'corpus.pckl', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "cell #29"
    ]
   },
   "outputs": [],
   "source": [
    "# opening the saved variables for reuse\n",
    "corpus = open_variable('Variables/', 'corpus.pckl')\n",
    "id2word = open_variable('Variables/', 'id2word.pckl')\n",
    "corrected_list_of_texts = open_variable('Variables/', 'corrected_list_of_texts.pckl')\n",
    "data_bigrams_trigrams = open_variable('Variables/', 'data_bigrams_trigrams.pckl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information and transparency purposes: Saving general information on the corpus, creating a DataFrame from it and exporting it as csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "cell #30"
    ]
   },
   "outputs": [],
   "source": [
    "# how many texts are in each year's corpus taken into account?\n",
    "number_pdf_docs = [len(sublist) for sublist in doc_names_pdf]\n",
    "number_xml_docs = [len(sublist) for sublist in doc_names_xml]\n",
    "number_docs = number_pdf_docs + number_xml_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": [
     "cell #31"
    ]
   },
   "outputs": [],
   "source": [
    "# corpus statistics\n",
    "statistics = pd.DataFrame([document_statistics, number_docs, count_english_texts], index=[\"Total No. of Documents\", \"No. of Documents After Filtering\", \"Documents in English\"], \n",
    "                   columns=['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2022', '2023'])\n",
    "statistics['Total'] = statistics.sum(axis=1)\n",
    "statistics = statistics.T\n",
    "statistics['% of English Texts'] = round(((100/statistics['No. of Documents After Filtering'])*statistics['Documents in English']), 2)\n",
    "statistics.to_csv('Figures/Statistics_Corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving some variables so that they can be used in the *MA_TopicModeling* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "cell #32"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'number_pdf_docs' (list)\n",
      "Stored 'number_xml_docs' (list)\n",
      "Stored 'number_docs' (list)\n",
      "Stored 'docnames' (list)\n",
      "Stored 'filenames_xml' (list)\n",
      "Stored 'filenames_pdf' (list)\n",
      "Stored 'filenames' (list)\n",
      "Stored 'all_freely_selectable_keywords' (list)\n",
      "Stored 'used_keywords_freely_selectable' (list)\n",
      "Stored 'used_keywords_predetermined' (list)\n",
      "Stored 'authors' (list)\n",
      "Stored 'authors_full_list' (list)\n"
     ]
    }
   ],
   "source": [
    "%store number_pdf_docs\n",
    "%store number_xml_docs\n",
    "%store number_docs\n",
    "%store docnames\n",
    "%store filenames_xml\n",
    "%store filenames_pdf\n",
    "%store filenames\n",
    "%store all_freely_selectable_keywords\n",
    "%store used_keywords_freely_selectable\n",
    "%store used_keywords_predetermined\n",
    "%store authors\n",
    "%store authors_full_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "265bbd40db63aa34df1bd83f77ecf498882faae903508c6e893ae6addaebaa43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
