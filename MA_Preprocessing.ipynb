{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code for Master's Thesis: Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. Welche Themen können mithilfe von Topic Modeling aus den DHd-Abstracts\n",
    "der Tagungen zwischen 2014 und 2023 gefunden werden?\n",
    "\n",
    "*Which topics can be found in the abstracts from DHd-conferences between 2014 and 2023 with Topic Modeling?*\n",
    "\n",
    "2. Welche Themen kommen häufig gemeinsam in einem Dokument vor und weisen\n",
    "daher eine hohe Themenähnlichkeit (topic similarity) auf?\n",
    "\n",
    "*Which topics appear frequently in one abstract and therefore have a high topic similarity?* **Hierarchical Clustering**\n",
    "\n",
    "3. Wie haben sich die Themenschwerpunkte im Verlauf der Jahre verändert -\n",
    "welche Trends sind zu erkennen?\n",
    "\n",
    "*How have the topics been changing throughout the years - which trends are perceptible?* **Mann-Kendall-Test**\n",
    "\n",
    "4. Welche Entwicklungen sind in Bezug auf die Verwendung verschiedener Forschungsmethoden festzustellen?\n",
    "\n",
    "*With regard to the use of different scientific methods, which developments are perceptible?*\n",
    "\n",
    "5. Welche Personen sind besonders häufig mit Abstracts vertreten, in welchen\n",
    "Autor:innenteams treten sie auf und wie verändern sich diese im Zeitverlauf?\n",
    "\n",
    "*Which researchers contribute to the conference particularly frequently with abstracts, in which teams do they contribute and how have the teams been changing?*\n",
    "\n",
    "6. Welche Personencluster sind in Bezug auf die Themenschwerpunkte zu erkennen und wie verändern sich diese?\n",
    "\n",
    "*Which clusters of researchers can be found with regard to topics and how have the clusters been changing?* **Network Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in necessary pdf- and xml-files\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "'''Vermerken: PyPDF2 hat die Zeichen nicht gut erkannt und daher sind einige Wörter herausgefallen'''\n",
    "import PyPDF2\n",
    "import fitz\n",
    "from io import BytesIO\n",
    "\n",
    "#(pre)processing the files\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from gensim.models import TfidfModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "''' In-Script Funktion hat nicht funktioniert '''\n",
    "from ocrfixr import spellcheck\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions: opening lists, saving and reopening objects, function to get conference names from file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list(doc_name):\n",
    "    file = open(doc_name, \"r\", encoding='utf-8')\n",
    "    data = file.read()\n",
    "\n",
    "    data = data.split(\", \")\n",
    "    return data\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(dirname, filename, varname):\n",
    "    filename = dirname + filename\n",
    "    g = open(filename, 'wb')\n",
    "    pickle.dump(varname, g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_variable(dirname, filename):\n",
    "    path = str(dirname) + str(filename)\n",
    "    f = open(path, 'rb')\n",
    "    filename = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conference_names(list):\n",
    "    files = []\n",
    "    for element in list:\n",
    "        new_name = element.split('.zip')[0]\n",
    "        new_name = new_name.split('Corpus/')[1]\n",
    "        new_name = re.sub('_', ' ', new_name)\n",
    "        files.append(new_name)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: determining text language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \n",
    "    #gets text as input\n",
    "    lang = detect(text)\n",
    "\n",
    "    #returns the language tag of detected language\n",
    "    return lang        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: Extracting text from XML-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_text(soup):\n",
    "    \n",
    "    # extract <p> tags from body of xml-document to find the actual text \n",
    "    document_body = soup.body\n",
    "    p_tags = document_body.find_all(\"p\")\n",
    "    \n",
    "    # return the text from p-tags\n",
    "    return p_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \n",
    "    # filtering paragraphs from text\n",
    "    clean = re.sub(r'\\n', \"\", str(text))\n",
    "\n",
    "\n",
    "    # filtering weblinks, digits and markup from XML\n",
    "    abbreviations = [r'http(.*?) ', r'\\d', r'<(.*?)>', r'https(.*?) ', r'www(.*?) ']\n",
    "    for word in abbreviations:\n",
    "        clean = re.sub(word, '', clean)\n",
    "    \n",
    "    # filtering punctuation\n",
    "    punctuation = '''!“()´`¨[]{}\\\\;:”\",<>/.?@#$%^&*_~''' \n",
    "    for word in clean:\n",
    "        if word in punctuation:\n",
    "            clean = clean.replace(word, \"\")\n",
    "    \n",
    "    # convert a document into a list of lowercase tokens, ignoring tokens that are too short (min_len=2) or too long (max_len=15), no deaccentation (by default)\n",
    "    clean = gensim.utils.simple_preprocess(clean, min_len=3, max_len=25)\n",
    "\n",
    "    # returns cleaned-up texts\n",
    "    return clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: removing stopwords and very short/long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, language, additional_stops):\n",
    "    \n",
    "    # import German stopword list \n",
    "    stops_de = set(stopwords.words('german'))\n",
    "    stops_de.update(additional_stops)\n",
    "    \n",
    "    stops_en = set(stopwords.words('english'))\n",
    "    stops_en.update(additional_stops)\n",
    "    \n",
    "    \n",
    "    # filter stopwords\n",
    "    words_filtered = []\n",
    "    for w in text:\n",
    "        if language == 'de':\n",
    "            if w not in stops_de:\n",
    "                words_filtered.append(w)\n",
    "\n",
    "        elif language == 'en':\n",
    "            if w not in stops_en:\n",
    "                words_filtered.append(w)\n",
    "    \n",
    "    # return list of words that are NOT stopwords\n",
    "    return words_filtered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: lemmatization with part-of-speech tagging\n",
    "- Lemmatizing the words in the texts to their dictionary form according to the detected language\n",
    "- Hint: 'de_core_news_md' and 'en_core_web_sm' models have to be downloaded via pip beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, language):\n",
    "    \n",
    "    # only words tagged as nouns, verbs, adjectives and adverbs should be considered\n",
    "    allowed_tags = ['NOUN', 'VERB', 'ADJ']\n",
    "\n",
    "    # disabling parser and ner-tool to accelerate computing \n",
    "    nlp_de = spacy.load('de_core_news_md', disable=['parser', 'ner'])\n",
    "    nlp_en = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        if language == 'de':\n",
    "            doc = nlp_de(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_tags:\n",
    "                    new_text.append(token.lemma_.lower())\n",
    "\n",
    "        elif language == 'en':\n",
    "            doc = nlp_en(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_tags:\n",
    "                    new_text.append(token.lemma_.lower())\n",
    "            \n",
    "        # delete all empty sets where the pos-tag was not in allowed list\n",
    "        if new_text != []:        \n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "    \n",
    "    # return list of lemmatized words\n",
    "    return (texts_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function: Extract List Items from Textfile After Manual OCR-Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_ocr(textfile):\n",
    "\n",
    "    pprocessed_pdf = []\n",
    "    pdf_ocr_corr = textfile.split(r']')\n",
    "    for item in pdf_ocr_corr:\n",
    "        item = re.sub(r', \\[', '', item)\n",
    "        item = re.sub(r'\\ufeff', '', item)\n",
    "        item = re.sub(r'\\[', '', item)\n",
    "        item = re.sub(r'\\'', '', item)\n",
    "        item = re.sub(r'\\’', '', item)\n",
    "        item = re.sub(r'\\‘', '', item)\n",
    "        clean = item.split(', ')\n",
    "        pprocessed_pdf.append(clean)\n",
    "    # cutting off the last item as it is not a text item\n",
    "    pprocessed_pdf = pprocessed_pdf[:-1]\n",
    "    \n",
    "    return pprocessed_pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: Making bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts, bigram):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts, trigram, bigram):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams_trigrams(texts):\n",
    "   \n",
    "    bigram_phrases = gensim.models.Phrases(texts, min_count=8, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[texts], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    data_bigrams = make_bigrams(texts, bigram)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams, trigram, bigram)\n",
    "\n",
    "    return data_bigrams_trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function: TF-IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(id2word, texts):\n",
    "    # simple bag of words for each document, containing tuples with (index, number of appearances of the word in the document)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # calculates term frequency (TF) weighted by the inverse document frequency (IDF) for every word/index in the bag of words\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    # low_value as threshold\n",
    "    low_value = 0.03\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "\n",
    "    # for every single bag of words\n",
    "    for i in range(0, len(corpus)):\n",
    "        # consider each bow for each document\n",
    "        bow = corpus[i]\n",
    "        \n",
    "        # for each tuple (index, tfidf-value) in the tf-idf-weighted bag of words, extract index (tfidf_ids)\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        \n",
    "        # for each tuple (index, bow-value without tfidf), extract index\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        \n",
    "        # if the value in the (index, tfidf-value) tuple is lower than 0.03, put id into list low_value_words\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        \n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        \n",
    "        # which words will be deleted from the bow?\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "    \n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf score 0 will be missing\n",
    "        \n",
    "        # add words which indexes are not in low_value_words and not in words_missing_in_tfidf to the new bag of words \n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        \n",
    "        # new bow is missing certain indexes\n",
    "        corpus[i] = new_bow\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Answering the Research Questions: Extracting Keywords from XML-File\n",
    "- extracts tags \\<keywords n=\"topics\" scheme=\"ConfTool\"> and \\<keywords n=\"keywords\" scheme=\"ConfTool\"> to get keywords of the texts\n",
    "- checks validity of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    \n",
    "    # removing xml markup and items shorter than three characters\n",
    "    keywords = re.sub(\"<(.*?)>\", \"\", keywords)\n",
    "    keywords = keywords.split(\"\\n\")\n",
    "    for item in keywords:\n",
    "        if len(item) <= 2:\n",
    "            keywords.remove(item)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(xmldata, conf_tool_methods):\n",
    "    \n",
    "    # finds all tags <keywords n=\"keywords\"> and <keywords n=\"topics\">, removes all tags within\n",
    "    keywords_free = str(xmldata.find_all('keywords', n='keywords'))\n",
    "    keywords_conf = str(xmldata.find_all('keywords', n='topics'))\n",
    "    \n",
    "    keywords_free = clean_keywords(keywords_free)\n",
    "    keywords_conf = clean_keywords(keywords_conf)\n",
    "    \n",
    "    # validity check: is the item really a keyword mentioned in the list?\n",
    "    for item in keywords_conf:\n",
    "        if item not in conf_tool_methods:\n",
    "            keywords_conf.remove(item)\n",
    "            \n",
    "    # returns list\n",
    "    return keywords_free, keywords_conf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Answering the Research Questions: Extracting the author names\n",
    "Extracts the names of the authors and returns a list of lists containing the names of the single texts' authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(title_stmt):\n",
    "    # returns a list of authors for each of the texts\n",
    "    \n",
    "    # navigating to the title statement and finding all tags <author>\n",
    "    authors = title_stmt.find_all(\"author\")\n",
    "    fore_and_surnames = []\n",
    "    \n",
    "    # extracting the <surname> and <forename> tags and cleaning the outcome from the tags and the brackets\n",
    "    for element in authors:\n",
    "        names = element.find_all(['surname', 'forename'])\n",
    "        names =  re.sub(\"<(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(\"</(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(r'\\]', \"\", names)\n",
    "        names = re.sub(r'\\[', \"\", names)\n",
    "        fore_and_surnames.append(names)\n",
    "    \n",
    "    return fore_and_surnames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating repositories in which variables, models and figures can be saved later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Variables/\"):\n",
    "    os.mkdir('Variables/')\n",
    "    print('Created new directory: Variables')\n",
    "\n",
    "rqs = ['RQ1', 'RQ2', 'RQ3', 'RQ4', 'RQ5', 'RQ6', ]    \n",
    "for question in rqs:\n",
    "    if not os.path.isdir('Figures/'+question):\n",
    "        os.mkdir('Figures/'+question)\n",
    "        print('Created new directory: Figures/', question )\n",
    "    \n",
    "if not os.path.isdir('Models/'):\n",
    "    os.mkdir('Models/')\n",
    "    print('Created new directory: Models')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in zip-files of DHd-conferences where only PDF-files are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_pdf = ['Corpus/DHd_2014.zip', 'Corpus/DHd_2015.zip']\n",
    "doc_statistics = []\n",
    "\n",
    "# extracting text from pdf-files\n",
    "all_pdf_texts = []\n",
    "doc_names_pdf = []\n",
    "\n",
    "#creating a list containing 9 zeros, which is filled with statistical information in the process of running the code\n",
    "en_count = [0] * 9\n",
    "k = 0\n",
    "\n",
    "for conference_file in filenames_pdf:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    doc_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.pdf':\n",
    "            doc_names_year.append(name)\n",
    "            pdf_data = BytesIO(archive.read(name))\n",
    "            # reading each pdf-file in the zip-archive\n",
    "            with fitz.open(stream=pdf_data, filetype='pdf') as doc:\n",
    "                text = ''\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "\n",
    "                all_pdf_texts.append(text)\n",
    "                lang = detect_language(text)\n",
    "                if lang == 'en':\n",
    "                    en_count[k] += 1\n",
    "\n",
    "\n",
    "    doc_names_pdf.append(doc_names_year)\n",
    "    k += 1\n",
    "    \n",
    "\n",
    "filenames_pdf = get_conference_names(filenames_pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the zip-files of the DHd-Conferences where XML-files were published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_xml = ['Corpus/DHd_2016.zip', 'Corpus/DHd_2017.zip', 'Corpus/DHd_2018.zip', 'Corpus/DHd_2019.zip', 'Corpus/DHd_2020.zip',\n",
    "             'Corpus/DHd_2022.zip', 'Corpus/DHd_2023.zip']\n",
    "\n",
    "\n",
    "\n",
    "all_xml_files = []\n",
    "doc_names_xml = []\n",
    "# read in all zip-folders\n",
    "for conference_file in filenames_xml:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    xml_per_year = []\n",
    "    # read in all files in the zip-file and check that they are xml-files\n",
    "    doc_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.xml' and not name[-9:] == 'final.xml':\n",
    "            xml_per_year.append(archive.read(name))\n",
    "            doc_names_year.append(name)\n",
    "    all_xml_files.append(xml_per_year)\n",
    "    # creating a list of all documents' names\n",
    "    doc_names_xml.append(doc_names_year)\n",
    "   \n",
    "\n",
    "docnames = doc_names_pdf + doc_names_xml\n",
    "filenames_xml = get_conference_names(filenames_xml)\n",
    "filenames = filenames_pdf + filenames_xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML-Files: \n",
    "\n",
    "The XML-files are not only used for text extraction, but since they contain a lot of information due to the extensive markup, some other information will be extracted from the files in the following steps:\n",
    "- Text \n",
    "- Authors of the documents\n",
    "- Keywords given in the metadata of the abstracts in order to find the scientific methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xml_texts = []\n",
    "\n",
    "# importing the list provided, which contains all selectable options for <keywords n='keywords'>\n",
    "conf_tool_methods = open_list('Misc/conf_tool_methods.txt')\n",
    "\n",
    "# contains a list per year, this list contains a list of keywords extracted per text\n",
    "all_free_keywords = []\n",
    "used_keywords_free = []\n",
    "used_keywords_conf = []\n",
    "authors = []\n",
    "authors_full_list = []\n",
    "\n",
    "for year in all_xml_files:\n",
    "    keywords_free_year = []\n",
    "    keywords_conf_year = []\n",
    "    authors_year = []\n",
    "    for doc in year:\n",
    "        \n",
    "        soup = BeautifulSoup(doc, 'xml')\n",
    "        \n",
    "        # Code for extracting the actual text from xml-files\n",
    "        xml_text = extract_xml_text(soup)\n",
    "        all_xml_texts.append(xml_text)\n",
    "        \n",
    "        lang = detect_language(str(xml_text))\n",
    "        if lang == 'en':\n",
    "            en_count[k] += 1\n",
    "        \n",
    "        \n",
    "        # Code for extracting the author names       \n",
    "        title_stmt = soup.titleStmt\n",
    "        authors_in_doc = extract_authors(title_stmt)\n",
    "        authors_year.append(authors_in_doc) \n",
    "\n",
    "        \n",
    "        # Code for extracting the keywords used in xml-files  (per year)\n",
    "        keywords_free, keywords_conf = extract_keywords(soup, conf_tool_methods)  \n",
    "        keywords_free_year = keywords_free_year + keywords_free\n",
    "        keywords_conf_year = keywords_conf_year + keywords_conf\n",
    "        \n",
    "        \n",
    "    # saving all keywords that were given in the <keyword n=keyword> tags in the XML-files\n",
    "    all_free_keywords = list(dict.fromkeys(all_free_keywords + keywords_free_year))\n",
    "    used_keywords_conf.append(keywords_conf_year)    \n",
    "    used_keywords_free.append(keywords_free_year)     \n",
    "    \n",
    "    # saves each text's authors in a list, sorted by year of the text\n",
    "    authors.append(authors_year)\n",
    "    \n",
    "    \n",
    "    for element in authors_year:\n",
    "        authors_full_list = authors_full_list + element\n",
    "    authors_full_list = list(dict.fromkeys(authors_full_list))\n",
    "    k += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the extracted PDF and XML texts for further processing of the textual content:\n",
    "\n",
    "- Cleaning up\n",
    "- Removing stopwords depending on the detected language (English or German)\n",
    "- Lemmatizing the texts depending on the detected language (English or German) --> time-consuming step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Do not exert if you have the variables stored!! '"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Do not exert if you have the variables stored!! '''\n",
    "whole_texts = []\n",
    "whole_texts = all_pdf_texts + all_xml_texts\n",
    "\n",
    "additional_stops = open_list('Misc/additional_stopwords.txt')\n",
    "\n",
    "list_all_texts = []\n",
    "for text in whole_texts:\n",
    "    # detecting language in order to remove the stopwords and lemmatize according to language\n",
    "    lang = detect_language(str(text))   \n",
    "    text_item = clean_text(text)\n",
    "    text_item = lemmatization(text_item, lang)\n",
    "    text_item = remove_stopwords(text_item, lang, additional_stops)\n",
    "    list_all_texts.append(text_item)\n",
    "\n",
    "# save_object('Variables/', 'list_all_texts.pckl', list_all_texts)\n",
    "''' Do not exert if you have the variables stored!! '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a txt-file *to_correct_ocr* containing the retrieved texts from the pdf-files (the first 231 ones) to manually postprocess/clean possible OCR mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Misc/to_correct_ocr.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(str(list_all_texts[:231]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the post-processed file and eventually bringing all texts together again in variable *corr_list_of_texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the post-processed file\n",
    "with open('Misc/ocr_correction.txt', 'r', encoding='utf-8') as p:\n",
    "    pdf_ocr_corr = p.read()\n",
    "# reading it \n",
    "corr_list_of_texts = post_processing_ocr(pdf_ocr_corr)\n",
    "corr_list_of_texts = corr_list_of_texts[:-1]\n",
    "\n",
    "# combining all 1203 lists/texts again\n",
    "list_all_texts = open_variable('Variables/', 'list_all_texts.pckl')\n",
    "corr_list_of_texts = corr_list_of_texts + list_all_texts[231:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams and trigrams, id2word and bag of words, which are necessary for the actual topic modeling algorithm (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bigrams and trigrams from lemmatized words\n",
    "data_bigrams_trigrams = create_bigrams_trigrams(corr_list_of_texts)\n",
    "\n",
    "# id2word as dictionary where every word/bi-/trigram is referenced with id\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "\n",
    "# corpus as dictionary that contains a list of tuples for each document, tuples contain (word id, no. of appearances of the word)\n",
    "# some index numbers are missing due to the tf-idf weighting \n",
    "corpus = tf_idf(id2word, data_bigrams_trigrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the variables for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the files to save the variables\n",
    "save_object('Variables/', 'corpus.pckl', corpus)\n",
    "save_object('Variables/', 'id2word.pckl', id2word)\n",
    "save_object('Variables/', 'data_bigrams_trigrams.pckl', data_bigrams_trigrams)\n",
    "save_object('Variables/', 'corr_list_of_texts.pckl', corr_list_of_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information/transparency purposes: Saving information on the corpus (e.g. for mentioning in the Thesis text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many texts are in each year's corpus taken into account?\n",
    "number_pdf_docs = [len(sublist) for sublist in doc_names_pdf]\n",
    "number_xml_docs = [len(sublist) for sublist in doc_names_xml]\n",
    "number_docs = number_pdf_docs + number_xml_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus statistics\n",
    "statistics = pd.DataFrame([doc_statistics, number_docs, en_count], index=[\"Total No. of Documents\", \"No. of Documents Ffter Filtering\", \"Documents in English\"], \n",
    "                   columns=['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2022', '2023'])\n",
    "statistics.to_csv('Figures/Statistics_Corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving some variables so that they can be used in the *MA_TopicModeling* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'number_pdf_docs' (list)\n",
      "Stored 'number_xml_docs' (list)\n",
      "Stored 'number_docs' (list)\n",
      "Stored 'docnames' (list)\n",
      "Stored 'filenames_xml' (list)\n",
      "Stored 'filenames_pdf' (list)\n",
      "Stored 'filenames' (list)\n",
      "Stored 'all_free_keywords' (list)\n",
      "Stored 'used_keywords_free' (list)\n",
      "Stored 'used_keywords_conf' (list)\n",
      "Stored 'authors' (list)\n",
      "Stored 'authors_full_list' (list)\n"
     ]
    }
   ],
   "source": [
    "%store number_pdf_docs\n",
    "%store number_xml_docs\n",
    "%store number_docs\n",
    "%store docnames\n",
    "%store filenames_xml\n",
    "%store filenames_pdf\n",
    "%store filenames\n",
    "%store all_free_keywords\n",
    "%store used_keywords_free\n",
    "%store used_keywords_conf\n",
    "%store authors\n",
    "%store authors_full_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "265bbd40db63aa34df1bd83f77ecf498882faae903508c6e893ae6addaebaa43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
