{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code for Master's Thesis: Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions X\n",
    "\n",
    "1. Welche Themen können mithilfe von Topic Modeling aus den DHd-Abstracts\n",
    "der Tagungen zwischen 2014 und 2023 gefunden werden?\n",
    "\n",
    "*Which topics can be found in the abstracts from DHd-conferences between 2014 and 2023 with Topic Modeling?*\n",
    "\n",
    "2. Welche Themen kommen häufig gemeinsam in einem Dokument vor und weisen\n",
    "daher eine hohe Themenähnlichkeit (topic similarity) auf?\n",
    "\n",
    "*Which topics appear frequently in one abstract and therefore have a high topic similarity?* **Hierarchical Clustering**\n",
    "\n",
    "3. Wie haben sich die Themenschwerpunkte im Verlauf der Jahre verändert -\n",
    "welche Trends sind zu erkennen?\n",
    "\n",
    "*How have the topics been changing throughout the years - which trends are perceptible?* **Mann-Kendall-Test**\n",
    "\n",
    "4. Welche Entwicklungen sind in Bezug auf die Verwendung verschiedener Forschungsmethoden festzustellen?\n",
    "\n",
    "*With regard to the use of different scientific methods, which developments are perceptible?*\n",
    "\n",
    "5. Welche Personen sind besonders häufig mit Abstracts vertreten, in welchen\n",
    "Autor:innenteams treten sie auf und wie verändern sich diese im Zeitverlauf?\n",
    "\n",
    "*Which researchers contribute to the conference particularly frequently with abstracts, in which teams do they contribute and how have the teams been changing?*\n",
    "\n",
    "6. Welche Personencluster sind in Bezug auf die Themenschwerpunkte zu erkennen und wie verändern sich diese?\n",
    "\n",
    "*Which clusters of researchers can be found with regard to topics and how have the clusters been changing?* **Network Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "cell #1"
    ]
   },
   "outputs": [],
   "source": [
    "#Reading in necessary pdf- and xml-files\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "'''Vermerken: PyPDF2 hat die Zeichen nicht gut erkannt und daher sind einige Wörter herausgefallen'''\n",
    "import PyPDF2\n",
    "import fitz\n",
    "from io import BytesIO\n",
    "\n",
    "#(pre)processing the files\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from HanTa import HanoverTagger as ht\n",
    "from langdetect import detect\n",
    "from gensim.models import TfidfModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "''' In-Script Funktion hat nicht funktioniert '''\n",
    "from ocrfixr import spellcheck\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions: opening lists, saving and reopening objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "cell #2"
    ]
   },
   "outputs": [],
   "source": [
    "def open_list(doc_name):\n",
    "    f = open(doc_name, \"r\", encoding='utf-8')\n",
    "    data = f.read()\n",
    "\n",
    "    data = data.split(\", \")\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "cell #3"
    ]
   },
   "outputs": [],
   "source": [
    "def save_object(dirname, filename, varname):\n",
    "    filename = dirname + filename\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(varname, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "cell #4"
    ]
   },
   "outputs": [],
   "source": [
    "def open_variable(dirname, filename):\n",
    "    path = str(dirname) + str(filename)\n",
    "    f = open(path, 'rb')\n",
    "    filename = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    return filename "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### Extracting Text from XML-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "cell #5"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_xml_text(soup):\n",
    "    \n",
    "    # extract <p> tags from body of xml-document to find the actual document text \n",
    "    p_tags = soup.body.find_all(\"p\")\n",
    "    \n",
    "    return p_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining Text Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "cell #6"
    ]
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    return detect(text)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Texts\n",
    "\n",
    "Removing new lines, weblinks, digits, markup and punctuation. Further using gensim.utils.simple_preprocess which eliminates tokens shorter than 3 or longer than 30 characters, and returns lower-cased items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "cell #7"
    ]
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \n",
    "    clean_text = re.sub(r'\\n', \"\", str(text))\n",
    "\n",
    "    sequences_to_remove = [r'http(.*?) ', r'\\d', r'<(.*?)>', r'https(.*?) ', r'www(.*?) ', '-']\n",
    "    for item in sequences_to_remove:\n",
    "        clean_text = re.sub(item, '', clean_text)\n",
    "    \n",
    "    # filtering punctuation\n",
    "    punctuation = '''!“()´`¨[]{}\\\\;:”\",<>/.?@#$%^&*_~''' \n",
    "    for item in clean_text:\n",
    "        if item in punctuation:\n",
    "            clean_text = clean_text.replace(item, \"\")\n",
    "      \n",
    "    # convert a document into a list of lowercase tokens, ignoring tokens that are too short (min_len=3) or too long (max_len=30), no deaccentation (by default)\n",
    "    clean_text = gensim.utils.simple_preprocess(clean_text, min_len=3, max_len=30)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization with Part-of-Speech Tagging\n",
    "\n",
    "Lemmatizing words tagged with certain part of speech and according to the detected language. Hanover Tagger is used since it lemmatized better than spaCy tagger, especially on German words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "cell #8"
    ]
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, language):\n",
    "    \n",
    "    tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "    tagger_en = ht.HanoverTagger('morphmodel_en.pgz')\n",
    "    \n",
    "    allowed_tags = ['NN', 'NE', 'ADJ(A)', 'ADJ(D)', 'VV(INF)', 'VV(FIN)', 'VV(PP)', 'VA(INF)', 'VA(FIN)', 'VM(FIN)', 'VM(INF)']\n",
    "\n",
    "    if language == 'de':\n",
    "        lemmatized_text = []\n",
    "        for token in texts:\n",
    "            tagged_token = tagger_de.analyze(token)\n",
    "            if tagged_token[1] in allowed_tags:\n",
    "                lemmatized_text.append(tagged_token[0].lower())\n",
    "\n",
    "    elif language == 'en':\n",
    "        lemmatized_text=[]\n",
    "        for token in texts:\n",
    "            tagged_token = tagger_en.analyze(token)\n",
    "            if tagged_token[1] in allowed_tags:\n",
    "                lemmatized_text.append(tagged_token[0].lower())\n",
    "            \n",
    "    return lemmatized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "Removing stopwords contained in standard lists for German and English, as well as from own list set up especially for the corpus used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "cell #9"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, language, additional_stops):\n",
    "    \n",
    "    stopwords_de = set(stopwords.words('german'))\n",
    "    stopwords_de.update(additional_stops)\n",
    "    \n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    stopwords_en.update(additional_stops)\n",
    "    \n",
    "    filtered_text = []\n",
    "    for w in text:\n",
    "        if language == 'de':\n",
    "            if w not in stopwords_de:\n",
    "                filtered_text.append(w)\n",
    "\n",
    "        elif language == 'en':\n",
    "            if w not in stopwords_en:\n",
    "                filtered_text.append(w)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract List Items from Textfile After Manual OCR-Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "cell #10"
    ]
   },
   "outputs": [],
   "source": [
    "def post_processing_ocr(manually_corrected_file):\n",
    "\n",
    "    postprocessed_texts = []\n",
    "    for item in manually_corrected_file.split(r']'):\n",
    "        item = re.sub(r', \\[', '', item)\n",
    "        item = re.sub(r'\\ufeff', '', item)\n",
    "        item = re.sub(r'\\[', '', item)\n",
    "        item = re.sub(r'\\'', '', item)\n",
    "        item = re.sub(r'\\’', '', item)\n",
    "        item = re.sub(r'\\‘', '', item)\n",
    "        clean = item.split(', ')\n",
    "        postprocessed_texts.append(clean)\n",
    "        \n",
    "    # cutting off the last two items as they are not text    \n",
    "    postprocessed_texts = postprocessed_texts[:-2]\n",
    "    \n",
    "    return postprocessed_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating N-Grams\n",
    "\n",
    "Creating n-grams by using classes *Phrases* and *Phraser* provided by *gensim.models*. Those classes identify phrases within the texts which qualify for n-grams, given the minimum count and the threshold. *create_bigrams* and *create_trigrams* then return the actual n-grams to the text basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "cell #11"
    ]
   },
   "outputs": [],
   "source": [
    "def create_bigrams(texts, bigram):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def create_trigrams(texts, trigram, bigram):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "cell #12"
    ]
   },
   "outputs": [],
   "source": [
    "def create_bigrams_trigrams(texts):\n",
    "   \n",
    "    bigram_phrases = gensim.models.Phrases(texts, min_count=8, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[texts], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    data_bigrams = create_bigrams(texts, bigram)\n",
    "    data_bigrams_trigrams = create_trigrams(data_bigrams, trigram, bigram)\n",
    "\n",
    "    return data_bigrams_trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Weighting\n",
    "\n",
    "TF-IDF weighting eliminates terms which are very frequent, i.e. ubiquitous, in the documents and therefore might not be very important to the texts' meaning. The threshold determines which TF-IDF value the terms have to overcome in order to not be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "cell #13"
    ]
   },
   "outputs": [],
   "source": [
    "def tf_idf(id2word, texts):\n",
    "    \n",
    "    # simple bag of words for each document, containing tuples with (index, number of appearances of the word in the document)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # calculates term frequency (TF) weighted by the inverse document frequency (IDF) for every word/index in the bag of words\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    # low_value as threshold\n",
    "    threshold = 0.03\n",
    "    deleted_words  = []\n",
    "\n",
    "    # for every single bag of words\n",
    "    for i in range(0, len(corpus)):\n",
    "        # consider each bow for each document\n",
    "        bow = corpus[i]\n",
    "        \n",
    "        # if the value in the (index, tfidf-value) tuple is lower than 0.03, put id into list low_value_words\n",
    "        below_threshold_ids = [id for id, value in tfidf[bow] if value < threshold]\n",
    "\n",
    "        # which words will be deleted from the bow?\n",
    "        for id in below_threshold_ids:\n",
    "            deleted_words.append(id2word[id])\n",
    "        \n",
    "        # add words which indexes are not in low_value_words and not in words_missing_in_tfidf to the new bag of words \n",
    "        new_bow = [b for b in bow if b[0] not in below_threshold_ids]\n",
    "        \n",
    "        # new bow is missing certain indexes\n",
    "        corpus[i] = new_bow\n",
    "    \n",
    "    return corpus, deleted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering the Research Questions\n",
    "#### Extracting Conference Names from Zip-Files\n",
    "\n",
    "*get_conference_nams* extracts the conference name from the zip-file name so that the proper names can be used for referencing the conferences in e.g. visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "cell #14"
    ]
   },
   "outputs": [],
   "source": [
    "def get_conference_names(list):\n",
    "    \n",
    "    files = []\n",
    "    for element in list:\n",
    "        new_name = element.split('.zip')[0]\n",
    "        new_name = new_name.split('Corpus/')[1]\n",
    "        new_name = re.sub('_', ' ', new_name)\n",
    "        files.append(new_name)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Keywords from XML-File\n",
    "\n",
    "- *extract_keywords*\n",
    "\n",
    "extracts tags \\<keywords n='topics' scheme='ConfTool'> and \\<keywords n='keywords' scheme='ConfTool'> to get keywords of the texts\n",
    "\n",
    "- *remove_markup_and_short_keywords*\n",
    "\n",
    "cleans keywords from xml-markup and removes those shorter than three characters\n",
    "\n",
    "- *remove_invalid_keywords*\n",
    "\n",
    "checks validity of keywords from \\<keywords n='topics'> by looking of keywords can be found in list of predetermined keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "cell #15"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_markup_and_short_keywords(keywords):\n",
    "    \n",
    "    keywords = re.sub(\"<(.*?)>\", \"\", keywords)\n",
    "    keywords = keywords.split(\"\\n\")\n",
    "    for item in keywords:\n",
    "        if len(item) <= 2:\n",
    "            keywords.remove(item)\n",
    "            \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "cell #16"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_invalid_keywords(keywords_conflist, conf_tool_methods):\n",
    "\n",
    "    for item in keywords_conflist:\n",
    "        if item not in conf_tool_methods:\n",
    "            keywords_conflist.remove(item)\n",
    "    \n",
    "    return keywords_conflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "cell #17"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_keywords(xmldata, conf_tool_methods):\n",
    "    \n",
    "    # finds all tags <keywords n=\"keywords\"> and <keywords n=\"topics\">, removes all tags within\n",
    "    keywords_freely_selectable = str(xmldata.find_all('keywords', n='keywords'))\n",
    "    keywords_conflist = str(xmldata.find_all('keywords', n='topics'))\n",
    "    \n",
    "    keywords_freely_selectable = remove_markup_and_short_keywords(keywords_freely_selectable)\n",
    "    keywords_conflist = remove_markup_and_short_keywords(keywords_conflist)\n",
    "    \n",
    "    remove_invalid_keywords(keywords_conflist, conf_tool_methods)\n",
    " \n",
    "    return keywords_freely_selectable, keywords_conflist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Author Names\n",
    "*extract_authors* finds the names of the authors and returns a list of lists containing the names of the single texts' authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "cell #18"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_authors(title_stmt):\n",
    "    \n",
    "    # navigating to the title statement and finding all tags <author>\n",
    "    authors = title_stmt.find_all(\"author\")\n",
    "    fore_and_surnames = []\n",
    "    \n",
    "    # extracting the <surname> and <forename> tags and cleaning the outcome from the tags and the brackets\n",
    "    for element in authors:\n",
    "        names = element.find_all(['surname', 'forename'])\n",
    "        names =  re.sub(\"<(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(\"</(.*?)>\", \"\", str(names))\n",
    "        names = re.sub(r'\\]', \"\", names)\n",
    "        names = re.sub(r'\\[', \"\", names)\n",
    "        fore_and_surnames.append(names)\n",
    "    \n",
    "    return fore_and_surnames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating repositories in which variables, models and figures can be saved later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "cell #19"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Variables/\"):\n",
    "    os.mkdir('Variables/')\n",
    "    print('Created new directory: Variables')\n",
    "\n",
    "rqs = ['RQ1', 'RQ2', 'RQ3', 'RQ4', 'RQ5', 'RQ6', ]    \n",
    "for question in rqs:\n",
    "    if not os.path.isdir('Figures/'+question):\n",
    "        os.mkdir('Figures/'+question)\n",
    "        print('Created new directory: Figures/', question )\n",
    "    \n",
    "if not os.path.isdir('Models/'):\n",
    "    os.mkdir('Models/')\n",
    "    print('Created new directory: Models')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in zip-files of DHd-conferences where only PDF-files are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "cell #20"
    ]
   },
   "outputs": [],
   "source": [
    "filenames_pdf = ['Corpus/DHd_2014.zip', 'Corpus/DHd_2015.zip']\n",
    "document_statistics = []\n",
    "\n",
    "# extracting text from pdf-files\n",
    "all_pdf_texts = []\n",
    "doc_names_pdf = []\n",
    "\n",
    "#creating a list containing 9 zeros, which is filled with statistical information in the process of running the code\n",
    "count_english_texts = [0] * 9\n",
    "year_index = 0\n",
    "\n",
    "for conference_file in filenames_pdf:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    document_statistics.append(len(archive.namelist()))\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.pdf':\n",
    "            doc_names_year.append(name)\n",
    "            pdf_data = BytesIO(archive.read(name))\n",
    "            # reading each pdf-file in the zip-archive\n",
    "            with fitz.open(stream=pdf_data, filetype='pdf') as doc:\n",
    "                text = ''\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "\n",
    "                all_pdf_texts.append(text)\n",
    "                # detecting text language here for statistics\n",
    "                lang = detect_language(text)\n",
    "                if lang == 'en':\n",
    "                    count_english_texts[year_index] += 1\n",
    "\n",
    "\n",
    "    doc_names_pdf.append(doc_names_year)\n",
    "    year_index += 1\n",
    "    \n",
    "\n",
    "filenames_pdf = get_conference_names(filenames_pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the zip-files of the DHd-Conferences where XML-files were published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "cell #21"
    ]
   },
   "outputs": [],
   "source": [
    "filenames_xml = ['Corpus/DHd_2016.zip', 'Corpus/DHd_2017.zip', 'Corpus/DHd_2018.zip', 'Corpus/DHd_2019.zip', 'Corpus/DHd_2020.zip',\n",
    "             'Corpus/DHd_2022.zip', 'Corpus/DHd_2023.zip']\n",
    "\n",
    "all_xml_files = []\n",
    "doc_names_xml = []\n",
    "# read in all zip-folders\n",
    "for conference_file in filenames_xml:\n",
    "    archive = zipfile.ZipFile(conference_file, 'r')\n",
    "    doc_names_year = []\n",
    "    xml_per_year = []\n",
    "    document_statistics.append(len(archive.namelist()))\n",
    "    # read in all files in the zip-file and check that they are xml-files\n",
    "    # exclude final.xml since those are not documents about a presentation\n",
    "    for name in archive.namelist():\n",
    "        if name[-4:] == '.xml' and not name[-9:] == 'final.xml':\n",
    "            xml_per_year.append(archive.read(name))\n",
    "            doc_names_year.append(name)\n",
    "    all_xml_files.append(xml_per_year)\n",
    "    # creating a list of all documents' names\n",
    "    doc_names_xml.append(doc_names_year)\n",
    "   \n",
    "\n",
    "docnames = doc_names_pdf + doc_names_xml\n",
    "filenames_xml = get_conference_names(filenames_xml)\n",
    "filenames = filenames_pdf + filenames_xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML-Files: \n",
    "\n",
    "The XML-files are not only used for text extraction, but since they contain a lot of information due to the extensive markup, some other information will be extracted from the files in the following steps:\n",
    "- Text \n",
    "- Authors of the documents\n",
    "- Keywords given in the metadata of the abstracts in order to find the scientific methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "cell #22"
    ]
   },
   "outputs": [],
   "source": [
    "all_xml_texts = []\n",
    "\n",
    "# importing the list provided, which contains all selectable options for <keywords n='keywords'>\n",
    "list_predetermined_keywords = open_list('Misc/predetermined_keywords.txt')\n",
    "\n",
    "# contains a list per year, this list contains a list of keywords extracted per text\n",
    "all_freely_selectable_keywords = []\n",
    "used_keywords_freely_selectable = []\n",
    "used_keywords_predetermined = []\n",
    "authors = []\n",
    "authors_full_list = []\n",
    "\n",
    "for year in all_xml_files:\n",
    "    keywords_freely_selectable_year = []\n",
    "    keywords_predetermined_year = []\n",
    "    authors_year = []\n",
    "    for doc in year:\n",
    "        \n",
    "        soup = BeautifulSoup(doc, 'xml')\n",
    "        \n",
    "        # Code for extracting the actual text from xml-files\n",
    "        xml_text = extract_xml_text(soup)\n",
    "        all_xml_texts.append(xml_text)\n",
    "        \n",
    "        lang = detect_language(str(xml_text))\n",
    "        if lang == 'en':\n",
    "            count_english_texts[year_index] += 1\n",
    "        \n",
    "        # Code for extracting the author names from titleStatement      \n",
    "        authors_in_doc = extract_authors(soup.titleStmt)\n",
    "        authors_year.append(authors_in_doc) \n",
    "\n",
    "        # Code for extracting the keywords used in xml-files  (per year)\n",
    "        keywords_freely_selectable, keywords_predetermined = extract_keywords(soup, list_predetermined_keywords)  \n",
    "        keywords_freely_selectable_year = keywords_freely_selectable_year + keywords_freely_selectable\n",
    "        keywords_predetermined_year = keywords_predetermined_year + keywords_predetermined\n",
    "        \n",
    "    # saving all keywords that were given in the <keyword n=keyword> tags in the XML-files\n",
    "    all_freely_selectable_keywords = list(dict.fromkeys(all_freely_selectable_keywords + keywords_freely_selectable_year))\n",
    "    used_keywords_predetermined.append(keywords_predetermined_year)    \n",
    "    used_keywords_freely_selectable.append(keywords_freely_selectable_year)     \n",
    "    \n",
    "    # saves each text's authors in a list, sorted by year of the text\n",
    "    authors.append(authors_year)\n",
    "    \n",
    "    for element in authors_year:\n",
    "        authors_full_list = authors_full_list + element\n",
    "    authors_full_list = list(dict.fromkeys(authors_full_list))\n",
    "    year_index += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the extracted PDF and XML texts for further processing of the textual content:\n",
    "\n",
    "- Determining text language\n",
    "- Cleaning the text from unneccessary contents\n",
    "- Lemmatizing the texts depending on the detected language (English or German) --> time-consuming step\n",
    "- Removing stopwords depending on the detected language (English or German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "cell #23"
    ]
   },
   "outputs": [],
   "source": [
    "# Time-consuming: Do not excute if you have the variables stored!\n",
    "\n",
    "whole_texts = []\n",
    "whole_texts = all_pdf_texts + all_xml_texts\n",
    "\n",
    "additional_stopwords = open_list('Misc/additional_stopwords.txt')\n",
    "\n",
    "list_all_texts = []\n",
    "for text in whole_texts:\n",
    "    # detecting language in order to remove the stopwords and lemmatize according to language\n",
    "    lang = detect_language(str(text)) \n",
    "    text = clean_text(text)\n",
    "    text = lemmatization(text, lang)\n",
    "    text = remove_stopwords(text, lang, additional_stopwords)\n",
    "    list_all_texts.append(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the list of all preprocessed texts and writing a file *to_correct_ocr.txt* containing the retrieved texts from the pdf-files to manually postprocess/clean possible OCR mistakes. The pdf-texts are the first 231 ones, therefore only those are written into the new txt-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "cell #24"
    ]
   },
   "outputs": [],
   "source": [
    "save_object('Variables/', 'list_all_texts.pckl', list_all_texts)\n",
    "\n",
    "with open('Misc/to_correct_ocr.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(list_all_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the post-processed file and eventually bringing all texts together again in variable *corr_list_of_texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "cell #25"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['glossarium', 'graecoarabicum', 'proposal', 'jahrestagung', 'yury', 'arzhanov', 'contact', 'ruhruniversitätbochum', 'philologie', 'sem', 'orientalistik', 'universitätsstr', 'bochum', 'email', 'yrarzhanovgmailcom', 'syriac', 'roeder', 'contact', 'berlinbrandenburgische', 'akademie', 'telota', 'jägerstraße', 'email', 'roederbbawde', 'musicology', 'language', 'literature', 'century', 'extent', 'activity', 'incorporation', 'reorganization', 'heritage', 'civilization', 'object', 'glossarium', 'graecoarabicum', 'research', 'glossarium', 'graecoarabicum', 'ruhruniversität', 'bochum', 'dfg', 'erc', 'database', 'glossarium', 'græcoarabicum', 'lexicon', 'reference', 'dictionary', 'lexicon', 'letter', 'jîm', 'end', 'alphabet', 'database', 'search', 'source', 'basis', 'effectiveness', 'database', 'link', 'library', 'online', 'coexistence', 'web', 'application', 'light', 'number', 'encoding', 'lexicon', 'brill', 'alphabet', 'context', 'representation', 'research', 'database', 'website', 'input', 'researcher', 'access', 'search', 'researcher', 'unicode', 'solution', 'user'], ['disambiguierung', 'suchtrefferlist', 'groß', 'textkorpora', 'anwendungsfeld', 'perspektive', 'thomas', 'alexander', 'christian', 'pölitz', 'angelika', 'storr', 'technisch', 'dortmund', 'institut', 'sprache', 'literatur', 'informatik', 'berlinbrandenburgisch', 'akademie', 'zentrum', 'sprache', 'zentrum', 'zeithistorisch', 'forschung', 'potsdam', 'zielsetzung', 'projekthintergrund', 'textkorpora', 'bieten', 'arbeitsbereich', 'neuartig', 'möglichkeit', 'forschungsfrage', 'authentisch', 'sprachverwendung', 'untersuchen', 'infrastrukturprojekt', 'bieten', 'flexibel', 'werkzeuge', 'datengewinnung', 'quantitativ', 'groß', 'linguistisch', 'strukturieren', 'textkorpora', 'auswerten', 'müssen', 'automatisch', 'gewonnen', 'manuell', 'nachbearbeiten', 'fall', 'wortform', 'sprachlich', 'zeichen', 'verbindung', 'form', 'inhalt', 'quantitativ', 'auswerten', 'sollen', 'homonym', 'polysem', 'textwort', 'aktuell', 'verfügbar', 'disambiguieren', 'unterscheidung', 'homonym', 'polysem', 'lesart', 'forschungsfrage', 'relevant', 'müssen', 'manuell', 'disambiguieren', 'verbunden', 'aufwand', 'erheblich', 'zeitlich', 'restriktion', 'forschungsprojekt', 'dissertation', 'studentisch', 'abschlussarbeit', 'bestimmt', 'fragestellung', 'bearbeiten', 'verbundprojekt', 'korpusbasiert', 'linguistisch', 'recherche', 'hilfe', 'kobra', 'arbeiten', 'germanistisch', 'linguistik', 'informatik', 'sprachtechnologie', 'sprachressourcenanbieter', 'gemeinsam', 'aufwand', 'manuell', 'nachbearbeitung', 'senken', 'möglichkeit', 'korpusbasiert', 'recherche', 'verbessern', 'dataminingverfahren', 'informatikpartner', 'aufgeben', 'aktuell', 'forschungsvorhaben', 'linguistik', 'anpassen', 'fallstudie', 'erproben', 'beteiligt', 'sprachtechnologie', 'sprachressourcenpartn', 'stellen', 'unterschiedlich', 'strukturieren', 'groß', 'textkorpora', 'bereit', 'wortartenannotiert', 'baumbank', 'integrieren', 'entwickelt', 'vorhanden', 'infrastruktur', 'fallstudie', 'beziehen', 'anwendungsfeld', 'korpusbasiert', 'lexikographie', 'diachronisch', 'sprachforschung', 'varietätenlinguistik', 'abgeleitet', 'aufgabenstellung', 'handeln', 'routineaufgabe', 'arbeit', 'groß', 'textkorpora', 'filtern', 'klassifizieren', 'disambiguieren', 'visualisieren', 'verschieden', 'arbeitsbereich', 'ähnlich', 'form', 'stellen', 'erläutern', 'ausgehend', 'konkret', 'anwendungsszenario', 'korpusbasiert', 'lexikographie', 'herausforderung', 'arbeit', 'groß', 'textkorpora', 'entstehen', 'leiten', 'anforderung', 'möglich', 'automatisch', 'anschlus', 'stellen', 'erst', 'erproben', 'verwenden', 'erzielen', 'dritt', 'schritt', 'zeigen', 'perspektivisch', 'analytisch', 'mehrwert', 'aufgabenstellung', 'forschung', 'verbundprojekt', 'bundesministerium', 'bildung', 'forschung', 'herbst', 'rahmen', 'programm', 'fördern', 'beteiligen', 'folgend', 'institution', 'projektleiter', 'dortmund', 'germanistik', 'angelika', 'storr', 'informatik', 'katharina', 'morik', 'berlinbrandenburgisch', 'akademie', 'alexander', 'eberhardkarlsuniversität', 'tübingen', 'erhard', 'hinrichs', 'institut', 'sprache', 'mannheim', 'marc', 'kupietzandrea', 'witt', 'fallstudie', 'anwendungsfeld', 'korpusbasiert', 'lexikographie', 'anwendungsszenario', 'wichtig', 'einsatzgebiet', 'textkorpora', 'lang', 'sprachlexikographie', 'engelberglemnitzer', 'referenzkorpus', 'dwdskernkorpus', 'hinblick', 'verteilung', 'enthalten', 'textbestand', 'textsortenbereich', 'belletristik', 'gebrauchsliteratur', 'journalistisch', 'prosa', 'dekade', 'jahrhundert', 'auswiegen', 'lexikographen', 'suchwort', 'automatisch', 'frequenzentwicklung', 'jahrhundert', 'gewinnen', 'gebräuchlichkeit', 'verschieden', 'textsortenbereich', 'vergleichen', 'aussagen', 'textsortenspezifik', 'bedeutungsentwicklung', 'speziell', 'wortbedeutung', 'treffen', 'möchten', 'müssen', 'ausgegeben', 'belegen', 'polysem', 'homonym', 'lexem', 'manuell', 'disambiguieren', 'anzahl', 'treff', 'suchwort', 'überschaubar', 'grenze', 'halten', 'fallen', 'storr', 'diskutiert', 'beispielwort', 'ampel', 'disambiguierung', 'vertretbar', 'zeitraufwand', 'möglich', 'untersuchen', 'beispielwort', 'leiter', 'resultieren', 'suchen', 'dwdskernkorpus', 'liste', 'belegen', 'vorhersehbar', 'anteilen', 'belegen', 'homonym', 'leiter', 'leiter', 'speziell', 'lesart', 'leiter', 'energieleiter', 'trittleiter', 'tonleiter', 'enthalten', 'lexem', 'belegzahl', 'hoch', 'manuell', 'disambiguierung', 'fällen', 'zeitlich', 'extrem', 'aufwändig', 'beschrieben', 'fallstudie', 'suchen', 'automatisch', 'disambiguierung', 'suchwort', 'belegen', 'korpusrecherchesystem', 'ausgeben', 'sollen', 'arbeiten', 'lexikographie', 'vereinfachen', 'verbessern', 'sollen', 'basis', 'statistisch', 'visualisierungswerkzeug', 'korpusdaten', 'kookkurrenzanalys', 'wortverlaufsdiagramm', 'überwiegend', 'formbasieren', 'arbeiten', 'komponente', 'semantisch', 'disambiguierung', 'anreichern', 'sollen', 'entwickeln', 'suchwort', 'monos', 'gelten', 'ungewöhnlich', 'neuartig', 'verwendung', 'tag', 'fördern', 'verwandt', 'arbeiten', 'vorgestellt', 'anwendungsszenario', 'liegen', 'reichweit', 'forschung', 'automatisch', 'disambiguierung', 'wortbedeutung', 'wordsensedisambiguation', 'wsd', 'zahlreich', 'arbeiten', 'widmen', 'breit', 'darstellen', 'überblick', 'agirren', 'umfangreich', 'vergleichsstudie', 'aktuell', 'navigli', 'veröffentlichen', 'scheinen', 'fall', 'zielführend', 'vorgegeben', 'lesart', 'disambiguieren', 'möglichkeit', 'korpus', 'potenziell', 'enthalten', 'unerwartet', 'lesart', 'entdecken', 'ausschließen', 'folgen', 'studie', 'ansatz', 'lesart', 'induktiv', 'korpusdaten', 'ermitteln', 'wordsenseinduction', 'wsi', 'liegen', 'reihe', 'erfolgreich', 'ansatz', 'wesentlich', 'clusteringverfahren', 'basieren', 'überblick', 'brodylapata', 'lapata', 'zeigen', 'mithilfe', 'latentdirichletallocation', 'lda', 'blei', 'tendenziell', 'erzielen', 'lassen', 'lda', 'basieren', 'annahme', 'verschieden', 'lesart', 'verwenden', 'unterschiedlich', 'kontext', 'vorkommen', 'vorkommen', 'behandelnd', 'kontextfenster', 'bestimmt', 'größe', 'legen', 'mithilfe', 'kookkurrenzstatistik', 'verteilung', 'kontextwort', 'sogenannt', 'ermitteln', 'lesart', 'auffassen', 'einzeln', 'kontextfenster', 'lassen', 'wahrscheinlichkeit', 'berechnen', 'bestimmt', 'vorkommen', 'behandelnd', 'bestimmt', 'lesart', 'zuordnen', 'annehmen', 'wahrscheinlichkeit', 'zuordnung', 'dirichletverteilung', 'folgen', 'rohrdantz', 'zeigen', 'nutzen', 'grundlage', 'visualisierung', 'bedeutungsentwicklung', 'beispielwort', 'zeitungskorpus', 'erlauben', 'entstehung', 'neubedeutung', 'entwicklung', 'rekonstruieren', 'erst', 'vorgestellt', 'fallstudie', 'lda', 'sprachdaten', 'dwdskernkorpus', 'jahrhundert', 'erproben', 'ergänzend', 'rohrdantz', 'weit', 'erkenntnis', 'nutzen', 'sprachdaten', 'gewinnen', 'unterschiedlich', 'textsortenbereich', 'stammen', 'beispiel', 'leiter', 'evaluieren', 'belegen', 'dwdskernkorpus', 'manuell', 'disambiguiert', 'belegen', 'verteilung', 'lesart', 'basis', 'kontextwört', 'bagofword', 'verhältnismäßig', 'groß', 'fenster', 'umfang', 'ganz', 'leiter', 'enthaltend', 'satz', 'ermitteln', 'reinheit', 'topicclust', 'hinblick', 'manuell', 'bestimmt', 'lesart', 'messen', 'messen', 'dienen', 'nmi', 'angewandt', 'ansatz', 'nmi', 'einfach', 'lloyd', 'nmi', 'überlegen', 'scheinen', 'erfordern', 'beschrieben', 'anwendungsszenario', 'weit', 'verfeinerung', 'veranschaulichen', 'lassen', 'ermitteln', 'häufig', 'manuell', 'bestimmt', 'lesart', 'zuordnen', 'bossführungsperson', 'politisch', 'leiter', 'ddrdritte', 'reich', 'leiter', 'bildungsinstitution', 'musikalisch', 'leiter', 'trittleiter', 'belegen', 'leiter', 'energie', 'tonleiter', 'selten', 'ermitteln', 'charakteristisch', 'kontextwört', 'verhältnismäßig', 'allgemein', 'salienz', 'gering', 'vergleich', 'syntaktisch', 'kookkurrenzstatistik', 'didakowskigeyk', 'verbesserung', 'gegenwärtig', 'unterschiedlich', 'kontextfenster', 'testen', 'weit', 'featureklassen', 'po', 'dependenzen', 'integrieren', 'ausführlich', 'beschreibung', 'auswertung', 'getestet', 'ansatz', 'paper', 'präsentieren', 'anwendungsmöglichkeit', 'disziplin', 'anwendungsbereich', 'beispiel', 'semantik', 'jahrhundert', 'lesart', 'grundlage', 'kontextwort', 'stellen', 'interessant', 'anwendungsmöglichkeit', 'geschichtswissenschaft', 'kontext', 'zusammenarbeiten', 'semantik', 'jahrhundert', 'ansiedeln', 'kollmeierhoffmann', 'kollmeiersaup', 'erscheinen', 'wandel', 'begreifen', 'verbunden', 'diskurs', 'beschäftigen', 'quantitativ', 'aufschlüsselung', 'polysem', 'begreifen', 'bedeutsam', 'gewinn', 'ermöglichen', 'schnell', 'zugriff', 'verschieden', 'bedeutung', 'verwendungsweis', 'relevant', 'begreifen', 'erleichtern', 'qualitativ', 'auswertung', 'textkorpora', 'rahmen', 'kooperation', 'sollen', 'beschrieben', 'weit', 'anwenden', 'sollen', 'basis', 'digitalisiert', 'zeitungskorpus', 'berliner', 'zeitung', 'sprachlich', 'ambiguität', 'zentral', 'begreifen', 'untersuchen', 'beispiel', 'begriff', 'einheit', 'verdeutlichen', 'stehen', 'ddr', 'verschwindend', 'maß', 'einheit', 'vorrangig', 'einheit', 'spd', 'kpd', 'einheit', 'wirtschaft', 'sozialpolitik', 'tauchen', 'begreifen', 'maßeinheit', 'produktionsberichterstattung', 'disambiguierungsverfahren', 'helfen', 'historischsemantisch', 'bedeutungswandel', 'begriff', 'einheit', 'ddr', 'empirisch', 'quantitativ', 'weisen', 'analysieren', 'literatur', 'agirren', 'wicentowski', 'proceedings', 'semeval', 'republic', 'blei', 'jordan', 'latent', 'dirichlen', 'journal', 'machine', 'lapata', 'bayesian', 'worden', 'proceedings', 'conferencen', 'chapt', 'computational', 'linguistic', 'stroudsburg', 'usa', 'didakowski', 'german', 'worden', 'profil', 'methodological', 'problem', 'solution', 'strategie', 'acces', 'structure', 'lexicographical', 'report', 'internet', 'mannheim', 'institut', 'sprache', 'publizieren', 'arbeiten', 'linguistik', 'engelberg', 'lemnitzer', 'einführung', 'lexikographie', 'wörterbuchbenutzung', 'tübingen', 'stauffenburg', 'corpu', 'reference', 'corpu', 'german', 'century', 'fellbaum', 'idiom', 'collocation', 'studies', 'london', 'kollmeier', 'erscheinen', 'ausgangspunkt', 'semantik', 'politisch', 'jahrhundert', 'kämper', 'warnke', 'diskurs', 'interdisziplinär', 'zugang', 'gegenstand', 'perspektive', 'diskursmuster', 'pattern', 'warnke', 'berlin', 'akademieverlag', 'kollmeier', 'hoffmann', 'roundtabl', 'geschichtlich', 'grundbegriff', 'writing', 'conceptual', 'history', 'twentieth', 'century', 'contribution', 'history', 'concept', 'kollmeier', 'hoffmann', 'zeitgeschichte', 'begreifen', 'perspektive', 'semantik', 'jahrhundert', 'debatte', 'zeithistorisch', 'forschung', 'studies', 'contemporary', 'history', 'raghavan', 'schützen', 'retrieval', 'cambridge', 'cambridge', 'press', 'mcenery', 'xiao', 'studies', 'resourcen', 'book', 'routledg', 'linguistic', 'london', 'new', 'york', 'routledg', 'navigli', 'worden', 'disambiguation', 'survey', 'survey', 'lloyd', 'leasen', 'square', 'transaction', 'theory', 'corpu', 'linguistic', 'international', 'handbook', 'band', 'berlin', 'new', 'york', 'gruyt', 'rohrdantz', 'toward', 'chang', 'analytic', 'proceedings', 'meeting', 'computational', 'linguistic', 'portland', 'oregon', 'storr', 'korpusgestützt', 'sprachanalyse', 'lexikographie', 'phraseologi', 'angewandt', 'linguistik', 'lehrbuch', 'tübingen', 'franck'], ['kostümsprache', 'mustersprache', 'analytisch', 'wert', 'formal', 'sprechen', 'muster', 'filmwissenschaft', 'johanna', 'barzen', 'frank', 'leymann', 'institut', 'architektur', 'anwendungssystem', 'iaa', 'stuttgart', 'barzen', 'leymanniaasunistuttgartd', 'kleidungssprache', 'formal', 'sprechen', 'medienwissenschaft', 'fragen', 'kostümsprache', 'film', 'greifbar', 'verstehbar', 'problem', 'präzise', 'definition', 'begriff', 'erweisen', 'schwierig', 'konzept', 'formal', 'sprache', 'informatik', 'nutzen', 'präzise', 'definition', 'geben', 'betrachten', 'bestandteil', 'kleidung', 'hose', 'hemd', 'genre', 'science', 'alphaben', 'möglich', 'kombination', 'kleidungsbestandteil', 'alphaben', 'kleidung', 'genre', 'auftreten', 'müssen', 'menge', 'möglich', 'einschränken', 'kombination', 'kleidungsbestandteil', 'genre', 'auftreten', 'filtern', 'ansatz', 'geschehen', 'produktionsregel', 'angeben', 'sinnvoll', 'kleidung', 'kleidungsbestandteil', 'zusammensetzen', 'anzug', 'bestehen', 'hose', 'weste', 'zeichen', 'anzug', 'hose', 'weste', 'alphaben', 'name', 'zusammengesetzt', 'kleidung', 'anzug', 'vokabular', 'grammatik', 'grammatik', 'erzeugen', 'sprache', 'kleidungssprache', 'genre', 'definition', 'grammatik', 'kleidung', 'genre', 'tupel', 'alphaben', 'vokabular', 'menge', 'produktionsregel', 'startsymbol', 'produktionsregel', 'vokabular', 'gelten', 'schreiben', 'menge', 'startelement', 'iterativ', 'anwendung', 'produktionsregel', 'grammatik', 'erzeugen', 'nennen', 'kleidungssprache', 'genre', 'bestandteil', 'kleidung', 'genre', 'produktionsregel', 'sinnvoll', 'genre', 'auftretend', 'kleidung', 'genau', 'repräsentativ', 'filmkorpus', 'genre', 'ableiten', 'entwickeln', 'kleidungsbestandteil', 'auftretend', 'kombination', 'kleidung', 'erfassen', 'kostümsprache', 'mustersprache', 'kleidung', 'kostüm', 'kostüm', 'kleidung', 'filmisch', 'intendiert', 'wirkung', 'festlegung', 'wirkung', 'kleidung', 'ansatz', 'abstrakt', 'wirkungsfunktion', 'wahr', 'falsch', 'repräsentieren', 'realisierung', 'funktion', 'praxis', 'geschehen', 'beispiel', 'schwellwert', 'häufigkeit', 'auftreten', 'kleidung', 'filmkorpus', 'festlegen', 'kleidung', 'schwellwert', 'überschreiten', 'kostüm', 'auszeichnen', 'experte', 'beurteilen', 'wirkung', 'formal', 'definieren', 'definition', 'menge', 'wahr', 'heißen', 'kostümsprache', 'genre', 'kostüm', 'bewährt', 'lösung', 'wiederkehrend', 'wirkungsproblem', 'auffassen', 'muster', 'sinn', 'software', 'architektur', 'verweisen', 'muster', 'lösung', 'problem', 'verfeinerung', 'komposition', 'beschreiben', 'sprechen', 'informatik', 'mustersprache', 'kostüm', 'verweisen', 'verschieden', 'bedeutung', 'gemeinsam', 'erscheinen', 'beschreiben', 'kombinierbarkeit', 'kostüm', 'kostümsprache', 'mustersprache', 'darstellbar', 'technik', 'informatik', 'umgang', 'mustern', 'lassen', 'kostüm', 'übertragen', 'informationssystem', 'kleidungssprache', 'kostümsprache', 'erfassung', 'kleidung', 'basieren', 'erstellt', 'umfangreich', 'taxonomie', 'kleidungsbestandteil', 'eigenschaft', 'taxonomie', 'notwendig', 'detailinformation', 'kleidung', 'film', 'beobachter', 'einpflegen', 'vergleichbar', 'geben', 'menge', 'taxonomie', 'kleidungsbestandteil', 'möglich', 'weit', 'farbnuance', 'erlauben', 'materialeigenschaft', 'beobachtbar', 'taxonomie', 'kontrollieren', 'erweiterbar', 'gegebenheit', 'filmkorpus', 'wesentlich', 'kleidungsbestandteil', 'eigenschaft', 'erfassen', 'beobachten', 'kombination', 'produktionsregel', 'kleidungsbestandteil', 'kleidung', 'genre', 'ende', 'erfassung', 'filmkorpus', 'grammatik', 'erstellen', 'filmkorpus', 'genre', 'erfassen', 'erfasst', 'kleidung', 'unterstützen', 'kostüm', 'identifizieren', 'wesentlich', 'variante', 'wirkungsfunktion', 'unterstützen', 'häufigkeitsanalyse', 'kostüm', 'hinweisen', 'experte', 'kleidung', 'begutachten', 'wirkung', 'bestätigen', 'ende', 'kostümsprache', 'genre', 'erstellen', 'ausblick', 'beitreten', 'experte', 'bewertung', 'forschungsergebnis', 'unterstützen', 'weit', 'arbeiten', 'steuern', 'helfen', 'etabliert', 'relevant', 'taxonomie', 'anforderung', 'mächtigkei', 'anfragen', 'bekannt', 'autor', 'mustersprache', 'achten', 'möglich', 'nutzung', 'weit', 'musterdomän']]\n"
     ]
    }
   ],
   "source": [
    "# opening the post-processed file\n",
    "with open('Misc/corrected_text.txt', 'r', encoding='utf-8') as f:\n",
    "    manually_corrected_file = f.read()\n",
    "\n",
    "corrected_list_of_texts = post_processing_ocr(manually_corrected_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "cell #26"
    ]
   },
   "outputs": [],
   "source": [
    "# # opening the saved variables for reuse\n",
    "\n",
    "# corpus = open_variable('Variables/', 'corpus.pckl')\n",
    "# id2word = open_variable('Variables/', 'id2word.pckl')\n",
    "# corrected_list_of_texts = open_variable('Variables/', 'corrected_list_of_texts.pckl')\n",
    "# data_bigrams_trigrams = open_variable('Variables/', 'data_bigrams_trigrams.pckl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams and trigrams, id2word and corpus, which are necessary for the actual topic modeling algorithm (LDA). For transparency and control purposes, the terms deleted by tf-idf weighting are written into a file and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "cell #27"
    ]
   },
   "outputs": [],
   "source": [
    "# creating bigrams and trigrams from lemmatized words\n",
    "data_bigrams_trigrams = create_bigrams_trigrams(corrected_list_of_texts)\n",
    "\n",
    "# id2word as dictionary where every word/bi-/trigram is referenced with id\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "# corpus as dictionary that contains a list of tuples for each document, tuples contain (word id, no. of appearances of the word)\n",
    "# some index numbers are missing due to the tf-idf weighting \n",
    "corpus, deleted_words = tf_idf(id2word, data_bigrams_trigrams)\n",
    "\n",
    "with open('Misc/tfidf_deleted_terms.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(deleted_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "cell #28"
    ]
   },
   "outputs": [],
   "source": [
    "# writing the files to save the variables\n",
    "save_object('Variables/', 'corrected_list_of_texts.pckl', corrected_list_of_texts)\n",
    "save_object('Variables/', 'data_bigrams_trigrams.pckl', data_bigrams_trigrams)\n",
    "save_object('Variables/', 'id2word.pckl', id2word)\n",
    "save_object('Variables/', 'corpus.pckl', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information and transparency purposes: Saving general information on the corpus, creating a DataFrame from it and exporting it as csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "cell #29"
    ]
   },
   "outputs": [],
   "source": [
    "# how many texts are in each year's corpus taken into account?\n",
    "number_pdf_docs = [len(sublist) for sublist in doc_names_pdf]\n",
    "number_xml_docs = [len(sublist) for sublist in doc_names_xml]\n",
    "number_docs = number_pdf_docs + number_xml_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": [
     "cell #30"
    ]
   },
   "outputs": [],
   "source": [
    "# corpus statistics\n",
    "statistics = pd.DataFrame([document_statistics, number_docs, count_english_texts], index=[\"Total No. of Documents\", \"No. of Documents Ffter Filtering\", \"Documents in English\"], \n",
    "                   columns=['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2022', '2023'])\n",
    "statistics.to_csv('Figures/Statistics_Corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving some variables so that they can be used in the *MA_TopicModeling* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "cell #31"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'number_pdf_docs' (list)\n",
      "Stored 'number_xml_docs' (list)\n",
      "Stored 'number_docs' (list)\n",
      "Stored 'docnames' (list)\n",
      "Stored 'filenames_xml' (list)\n",
      "Stored 'filenames_pdf' (list)\n",
      "Stored 'filenames' (list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'all_free_keywords'\n"
     ]
    }
   ],
   "source": [
    "%store number_pdf_docs\n",
    "%store number_xml_docs\n",
    "%store number_docs\n",
    "%store docnames\n",
    "%store filenames_xml\n",
    "%store filenames_pdf\n",
    "%store filenames\n",
    "%store all_free_keywords\n",
    "%store used_keywords_free\n",
    "%store used_keywords_conf\n",
    "%store authors\n",
    "%store authors_full_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "265bbd40db63aa34df1bd83f77ecf498882faae903508c6e893ae6addaebaa43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
